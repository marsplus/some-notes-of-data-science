\subsection{Common Activation Function}
    \begin{itemize}
        \item Sigmoid: it is rarely used in practice anymore because: 1) it is easily saturated at the left and right ends, which makes gradients zero; 2) the output is not zero-centered (it is $0.5$ centered), which is undesirable.
        \item Tanh: it is zero-centered, but the saturation issues still exist.
        \item ReLU: it is popular nowadays. However, it could make some neurons ``dead'', i.e., not activated in most of the training process.
        \item Some others including Leaky ReLU, Maxout, etc. See: \url{https://cs231n.github.io/neural-networks-1/#actfun}.
    \end{itemize}


\subsection{Optimization in Deep Learning}
    \subsubsection{Local Minimum/Maximum and Saddle Points}
    Let the loss function at a point $\bm{x}$ be $L(\bm{x})$.
    Suppose $\bm{x}$ is a point where the gradients become zeros, i.e., $\nabla_{\bm{x}}L(\bm{x}) = \bm{0}$.
    We can determine whether $\bm{x}$ is a local minimum/maximum from the Hessian matrix $\bm{H}$:
        \begin{itemize}
            \item If all eigenvalues of $\bm{H}$ are positive, then $\bm{x}$ is a local minimum.
            \item If all eigenvalues of $\bm{H}$ are negative, then $\bm{x}$ is a local maximum.
            \item If the eigenvalues of $\bm{H}$ include both positives and negatives, then $\bm{x}$ is a saddle point.
        \end{itemize}
        
    \subsubsection{Gradient Descent (GD) in One-dimensional Case}
    Consider a one-dimensional function: $f(x): \R \rightarrow \R$.
    Do a first-order Taylor expansion of $f(x + \epsilon)$ at $x$ gives us:
        \begin{equation}
                f(x + \epsilon) \approx f(x) + \epsilon f'(x) + O(\epsilon^2).
        \end{equation}
    We want to choose $\epsilon$ such that the value of $f(x + \epsilon)$ is decreased compared with the value of $f(x)$.
    A natural choice is $\epsilon = -\eta f'(x)$.
    The right-hand side becomes $f(x) - \eta (f'(x))^2 + O(\epsilon^2)$: when $\epsilon$ is not too large such that $O(\epsilon^2)$ is not significant, we have:
        \begin{equation}
            f(x - \eta f'(x)) \approx f(x) - \eta (f'(x))^2 \le f(x).
        \end{equation}
    This justifies the choice of setting $\epsilon$ to the negative of the first-order derivative.
    Notice that the value of the learning rate $\eta$ is important.
    If $\eta$ is too large then the second-order term $O(\epsilon^2)$ becomes significant, moving toward the direction $-\eta f'(x)$ may not decrease the function value.
    
    \subsubsection{Newton's Method}\label{newton-method}
        Consider a function $f(\bm{x}): \R^\nfeat \rightarrow \R$.
        We do a second-order Taylor expansion of $f(\bm{x} + \bm{\epsilon})$ at $f(\bm{x})$ as follows:
            \begin{equation}
                f(\bm{x} + \bm{\epsilon}) \approx f(\bm{x}) + \bm{\epsilon}^{\top} \nabla f(\bm{x}) + \frac{1}{2} \bm{\epsilon}^\top \nabla^2 f(\bm{x}) \bm{\epsilon}.
            \end{equation}
        At a minimum, we have $\nabla_{\bm{\epsilon}} f(\bm{x}+\bm{\epsilon}) = \bm{0}$; otherwise, there must be a direction $\bm{\epsilon}'$ where we can further minimize the function.
        Substituting into the Taylor expansion leads to 
            \begin{equation}
                \nabla_{\bm{\epsilon}}\left[ f(\bm{x}) + \bm{\epsilon}^{\top} \nabla f(\bm{x}) + \frac{1}{2} \bm{\epsilon}^\top \nabla^2 f(\bm{x}) \bm{\epsilon}\right] = \bm{0} \implies \bm{\epsilon} = -\bm{H}^{-1} \nabla f(\bm{x}).
            \end{equation}
        As a result, the update rule in Newton's method is as follows:
            \begin{equation}
                \bm{x} \leftarrow \bm{x} - \eta \bm{H}^{-1} \nabla f(\bm{x}),
            \end{equation}
        which shows how the Hessian matrix comes into play.
    
    
    
    \subsubsection{GD, SGD, and Mini-Batch SGD}
        The loss function is $L = \frac{1}{\ndata}\sum_{i=1}^{\ndata}{f(\bm{x}_i)}$.
        \begin{itemize}
            \item GD: compute the gradients over the entire dataset, i.e., $\nabla L = \sum_{i=1}^{\ndata}{\nabla f(\bm{x}_i)}$, which costs $O(\ndata)$ and might be too expensive when \ndata is large.
            \item SGD: randomly pick an index $i \in \SET{1, \ldots, \ndata}$ and compute $\nabla f(\bm{x}_i)$. This costs $O(1)$ but may oscillate a lot.
            \item Mini-batch SGD: instead of picking a single index, mini-batch SGD picks a subset $\mathcal{M} \subseteq \SET{1, \ldots, \ndata}$ uniformly at random and then computes $\sum_{i \in \mathcal{M}}^{}{\nabla f(\bm{x}_i)}$.
            There are two benefits of mini-batch SGD:
                \begin{itemize}
                    \item[1)] Mini-batch SGD converges much faster than GD.
                    This is because mini-batch SGD updates the model parameters more frequently than GD does in the same amount of time.
                    \item[2)] Compared with SGD, mini-batch SGD reduces variance by averaging the gradients over a mini-batch.
                \end{itemize}
            Note that when doing mini-batch SGD we need to shuffle the training data after every epoch.
            This is helpful for escaping local optima, which makes convergence faster.
        \end{itemize}
        
        \paragraph{Shuffling in SGD.}
        The standard way of shuffling in SGD:
            \begin{itemize}
                \item[1.] Before each epoch, re-shuffle the entire dataset.
                \item[2.] Sample mini-batchs without replacement.
            \end{itemize}
        Notice that the following strategies do \emph{not} work:
            \begin{itemize}
                \item[1.] Before each epoch, shuffle the order of mini-batches, while the data in each mini-batch stays the same.
                This is related to the Coupon Collector Problem: \url{shorturl.at/fnHXY}.
                Intuitively, given $K$ mini-batches, we only need $\Theta(K \log{K})$ epochs to traverse all the $K$ mini-batches, which could stuck the optimization into local optima.
                \item[2.] Only shuffle the data within each mini-batch. The gradient computed on each mini-batch is invariant to the order of the data, so this is actually equivalent to using a static order of the entire dataset.
            \end{itemize}
            
    
        
    \subsubsection{Common Optimizers in DL}

            \paragraph{Momentum.} 
            The key idea is to leverage historical gradients. Let $\bm{g}_t$ be the gradient at step $t$:
                \begin{equation}
                    \begin{aligned}
                        \bm{v}_t & = \beta  \bm{v}_{t-1} + \bm{g}_t \text{ (recursively taking past gradients into account)}\\
                        \bm{\theta}_t & = \bm{\theta}_{t-1} - \eta_t \bm{v}_t,
                    \end{aligned}
                \end{equation}
            where $\beta \in (0, 1)$. By tuning the value of $\beta$, we can take either long- or short-term gradient information into account.

            \paragraph{Adagrad.} 
            Recall from Section~\ref{newton-method} that the inverse of the Hessian matrix is used to precondition the gradients. 
            However, computing the Hessian and its inverse is prohibitively expensive. 
            Adagrad uses the following updating rule:
                \begin{equation}
                    \begin{aligned}
                        \bm{s}_t & = \bm{s}_{t-1} + \bm{g}_t^2  \text{ ($\bm{s}_t$ keeps track of the variance of the gradients)}\\
                        \bm{\theta}_t & = \bm{\theta}_{t-1} - \frac{\eta}{\sqrt{\bm{s}_t + \bm{\epsilon}}} \odot \bm{g}_t,
                    \end{aligned}
                \end{equation}
            where $\bm{g}_t^2$ is element-wise squares and $\bm{\epsilon} > \bm{0}$ ensures that we do not divide by zeros.
            The intuition is that we can use the variance of the gradients as a cheap proxy for the scale of the Hessian. 

            \paragraph{RMSProp.} 
            The $\bm{s}_t$ in Adagrad may grow very large. A simple fix of RMSProp is as follows with $\gamma \in (0, 1)$:
                \begin{equation}
                    \begin{aligned}
                        \bm{s}_t & = \gamma \bm{s}_{t-1} + (1 - \gamma) \bm{g}_t^2  \\
                        \bm{\theta}_t & = \bm{\theta}_{t-1} - \frac{\eta}{\sqrt{\bm{s}_t + \bm{\epsilon}}} \odot \bm{g}_t.
                    \end{aligned}
                \end{equation}
            Another benefit of RMSProp is that the learning rate $\eta$ is decoupled from the scaling of $\bm{g}_t$.


            \paragraph{Adam.} 
            The key idea is to keep track of both the momentum and the second-order moments of the gradients.
            The updating rule of Adam is as follows:
                \begin{equation}
                    \begin{aligned}
                        \bm{v}_t & = \beta_1 \bm{v}_{t-1} + (1 - \beta_1) \bm{g}_{t} \text{ (keep track of momentum)} \\
                        \bm{s}_t & = \beta_2 \bm{s}_{t-1} + (1-\beta_2) \bm{g}_t^2 \text{ (keep track of second-order moments)} \\
                        \hat{\bm{v}}_t & = \frac{\bm{v}_t}{1 - \beta_1^t}, \hat{\bm{s}}_t = \frac{\bm{s}_t}{1 - \beta_2^t} \text{ (re-normalization)} \\
                        \bm{\theta}_t & = \bm{\theta}_{t-1}  - \eta \frac{\hat{\bm{v}}_t}{\sqrt{\hat{\bm{s}}_t} +\bm{\epsilon}}.
                    \end{aligned}
                \end{equation}

        
    


\subsection{Variational Autoencoder (VAE)}
Let $\bm{x}$ represent the input data and $\bm{z}$ the hidden values. 
VAE can be interpreted from a Bayesian perspective as follows:
    \begin{enumerate}
        \item We sample a hidden vector $\bm{z}$ from its prior distribution, i.e., $\bm{z} \sim p_{\bm{\theta}}(\bm{z})$.
        \item We generate a output $\bm{x}$ by sampling from the posterior distribution conditioned on the hidden vector from step 1:  $\bm{x} \sim p_{\bm{\theta}}(\bm{x} | \bm{z})$.
    \end{enumerate}
With this two-step framework, a standard way to learn $\bm{\theta}$ is maximum likelihood estimation:
    \begin{equation}
        \bm{\theta}^\ast \in \argmax_{\bm{\theta}} \underbrace{ \int_{}^{}{p_{  \bm{\theta}}(\bm{x} | \bm{z}) p_{\bm{\theta}}(\bm{z})} d \bm{z} }_{\text{the likelihood of $\bm{x}$, i.e., $p_{\bm{\theta}}(\bm{x})$}}.
    \end{equation}
However, the integral is hard to evaluate as enumerating all $\bm{z}$ is difficult.
Instead, the VAE paper does the following:
    \begin{itemize}
        \item The encoder: learn a distribution $q_{\bm{\phi}}(\bm{z} | \bm{x})$, which enables us to sample a hidden vector $\bm{z}$ given an input $\bm{x}$.
        This is an application of \emph{variational inference}, i.e., using a parameterized function (i.e., $q_{\bm{\phi}}(\bm{z} | \bm{x})$) to approximate the true posterior $p(\bm{z} | \bm{x})$.
        \item The decoder: learn another distribution $p_{\bm{\theta}}(\bm{x} | \bm{z})$, which can be used to generate an output given the sampled hidden vector $\bm{z}$.
    \end{itemize}
The learning becomes maximizing the following:
    \begin{equation}\label{vae:obj}
        \log p_{\bm{\theta}}(\bm{x}) - D_\text{KL}(q_{\bm{\phi}}(\bm{z} | \bm{x}) || p_{\bm{\theta}}(\bm{z} | \bm{x})) = 
        \underbrace{\E_{\bm{z} \sim q_{\bm{\phi}}(\bm{z} | \bm{x})}\left[ p_{\bm{\theta}}(\bm{x} | \bm{z}) \right]}_{(\ast)} - \underbrace{D_\text{KL}(q_{\bm{\phi}}(\bm{z} | \bm{x}) || p_{\bm{\theta}}(\bm{z}))}_{(\diamond)}.
    \end{equation}
The left-hand side is called Evidence Lower Bound (ELBO), as it is a lower bound of the log-likelihood of $\bm{x}$, i.e.,
    \begin{equation}
        \log p_{\bm{\theta}}(\bm{x}) - D_\text{KL}(q_{\bm{\phi}}(\bm{z} | \bm{x})  ||  p_{\bm{\theta}}(\bm{x} | \bm{z})) \le \log p_{\bm{\theta}}(\bm{x}).
    \end{equation}

\paragraph{How to optimize the ELBO?} VAE is actually a model with both hidden variables (i.e., $\bm{z}$) and learnable parameters (i.e., those parameters $\bm{\theta}, \bm{\phi}$, etc.). 
A conventional approach to learn such a model is by using the famous EM algorithm. 
However, with the \emph{reparameterization trick} and the power of automatic differentiation tools, we can use gradient descent methods to learn a VAE.
A brief description of the optimization is as follows:
    \begin{itemize}
        \item To maximize $(\ast)$, we sample a set of hidden vectors $\bm{z}$ (the encoding part), which enables us to generate a set of vectors $\bm{x}^\prime$ through the decoder. We then minimize the $L_2$ distance between $\bm{x}$ and the sampled $\bm{x}^\prime$.
        \item The minimization of $(\diamond)$ may have closed-form solution; one example is given  in the VAE paper.
    \end{itemize}
Check out this fantastic blog for more details: \url{https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html}.
Another example implementation based on PyTorch is: \url{https://github.com/ethanluoyc/pytorch-vae/blob/master/vae.py}.

\subsection{RNN, GRU and LSTM}
This section is based on the excellent book: \url{shorturl.at/nwDEL}.
% Let us first consider a one-dimensional RNN encoded as follows:
%     \begin{equation}
%         \begin{aligned}
%             h_t & = f(x_t, h_{t-1}, w_h) \\
%             o_t & = g(h_t, w_o).
%         \end{aligned}
%     \end{equation}
% The first equation models the ``hidden-to-hidden'' transition, where $x_t$ is the input data to the hidden layer and $w_h$ the parameter of the transition.
% Notice that the parameter of the ``input-to-hidden'' transition is included into $w_h$ for now.
% The second equation models the ``hidden-to-output'' transition.
% The loss function evaluated on a $T$-step sequence is as follows:
%     \begin{equation}
%         L(x_1, \ldots, x_T, y_1, \ldots, y_T, w_h, w_o) = \frac{1}{T}\sum_{i=1}^{T}{\ell(y_t, o_t)}.
%     \end{equation}
% We need to compute two derivatives:
%     \begin{equation}
%         \begin{aligned}
%             \frac{\partial L}{\partial w_o} & = \frac{1}{T}\sum_{i=1}^{T}{\frac{\partial \ell(y_t, o_t)}{\partial w_o}} = \frac{1}{T}\sum_{i=1}^{T}{\frac{\partial \ell(y_t, o_t)}{\partial o_t} \frac{\partial g(h_t, w_o)}{\partial w_o}} \\
%             \frac{\partial L}{\partial w_h} & = \frac{1}{T}\sum_{i=1}^{T}{\frac{\partial \ell(y_t, o_t)}{\partial w_h}} = \frac{1}{T}\sum_{i=1}^{T}{\frac{\partial \ell(y_t, o_t)}{\partial o_t} \frac{\partial g(h_t, w_o)}{\partial h_t}} \frac{\partial h_t}{\partial w_h}.
%         \end{aligned}
%     \end{equation}
% The first derivative is direct to compute. 
% For the second derivative, the computation of $\frac{\partial h_t}{\partial w_h}$ is tricky, since it involves a recursive computation:
%     \begin{equation}\label{rnn:simple-grad}
%         \frac{\partial h_t}{\partial w_h} = \frac{\partial f(x_t, h_{t-1}, w_h)}{\partial w_h} + \frac{\partial f(x_t, h_{t-1}, w_h)}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial w_h},
%     \end{equation}
% where $h_{t-1}$ depends on $w_h$ through its previous hidden state $h_{t-2}$.
% We abstract the above recursive structure as three sequences $\SET{a_t}, \SET{b_t}, \SET{c_t}$, satisfying the following:
%     \begin{equation}\label{rnn:simple-series}
%         \begin{aligned}
%             a_t & = b_t + c_t \cdot a_{t-1} \\
%             a_0 & = 0,
%         \end{aligned}
%     \end{equation}
% where $a_t = \frac{\partial h_t}{\partial w_h}$, $b_t =  \frac{\partial f(x_t, h_{t-1}, w_h)}{\partial w_h}$ and $c_t = \frac{\partial f(x_t, h_{t-1}, w_h)}{\partial h_{t-1}}$.
% By setting $t=1$, the second term on the right-hand side of \eqref{rnn:simple-grad} should be zero, since there is no hidden state before $h_1$; this gives us the initial condition $a_0 = 0$.
% For any $t \ge 1$ we expand \eqref{rnn:simple-series}, which leads to:
%     \begin{equation}\label{rnn:key}
%         a_t = b_t + \sum_{i=1}^{t-1}{ \left( \prod_{j=i+1}^{t}{c_j} \right) b_i }.
%     \end{equation}
% The above formulate will help us a lot when we actually compute the gradients of RNN.

Consider a RNN with the following transitions:
    \begin{equation}
        \begin{aligned}
            \bm{h}_t & = \bm{W}_{hx} \bm{x}_t + \bm{W}_{hh} \bm{h}_{t-1} \\
            \bm{o}_t & = \bm{W}_{qh} \bm{h}_t.
        \end{aligned}
    \end{equation}
The loss funciton is $L=\frac{1}{T}\sum_{i=1}^{T}{\ell(y_t, \bm{o}_t)}$.
The tricky part is to compute $\frac{\partial L}{\partial \bm{h}_t}$.
When $t=T$, this is easy:
    \begin{equation}
        \frac{\partial L}{\partial \bm{h}_T} = \text{prod}\left( \frac{\partial L}{\partial \bm{o}_T}, \frac{\partial \bm{o}_T}{\partial \bm{h}_T}  \right) = \bm{W}_{qh}^\top \frac{\partial L}{\partial \bm{o}_T},
    \end{equation}
where $\frac{\partial L}{\partial \bm{o}_T}$ is direct to compute. 
When $t < T$, the loss $L$ depends on $\bm{h}_t$ and $\bm{h}_{t+1}$ (and recursively $\bm{h}_{t+2}$, etc.):
    \begin{equation}\label{rnn-grad}
        \begin{aligned}
            \frac{\partial L}{\partial \bm{h}_t} & = \text{prod}\left( \frac{\partial L}{\partial \bm{h}_{t+1}}, \frac{\partial \bm{h}_{t+1}}{\partial  \bm{h}_{t}} \right) + \text{prod}\left( \frac{\partial L}{\partial \bm{o}_t} , \frac{\partial \bm{o}_t}{\partial \bm{h}_t} \right) \\
            & = \bm{W}_{hh}^\top \frac{\partial L}{\partial \bm{h}_{t+1}} + \bm{W}_{qh}^\top \frac{\partial L}{\partial \bm{o}_t} \\
            & = \bm{W}_{hh}^\top\left( \bm{W}_{hh}^\top \frac{\partial L}{\partial \bm{h}_{t+2}} +  \bm{W}_{qh}^\top \frac{\partial L}{\partial \bm{o}_{t+1}} \right) + \bm{W}_{qh}^\top \frac{\partial L}{\partial \bm{o}_t} \\
            & = \bm{W}_{hh}^\top \bm{W}_{hh}^\top \left( \bm{W}_{hh}^\top \frac{\partial L}{\partial \bm{h}_{t+3}} +  \bm{W}_{qh}^\top \frac{\partial L}{\partial \bm{o}_{t+2}}\right) + \bm{W}_{hh}^\top \bm{W}_{qh}^\top \frac{\partial L}{\partial \bm{o}_{t+1}}  + \bm{W}_{qh}^\top \frac{\partial L}{\partial \bm{o}_t} \\
            & = \left( \bm{W}_{hh}^\top \right)^{T-t} \bm{W}_{qh}^\top \frac{\partial L}{\partial \bm{o}_T} + \left( \bm{W}_{hh}^\top \right)^{T-t-1}\bm{W}_{qh}^\top \frac{\partial L}{\partial \bm{o}_{T-1}} +, \ldots, + \left( \bm{W}_{hh}^\top \right)^{T-T} \bm{W}_{qh}^\top \frac{\partial L}{\partial \bm{o}_t} \\
            & = \sum_{i=t}^{T}{\left( \bm{W}_{hh}^\top \right)^{T-i} \bm{W}_{qh}^\top  } \left( \frac{\partial L}{\partial \bm{o}_{T+t-i} } \right).
        \end{aligned}
    \end{equation}
The so-called ``gradient explosion'' and ``gradient vanishing'' manifest themselves from the last step of \eqref{rnn-grad}.
\emph{When computing the gradients of the loss w.r.t. a hidden state, we need to repeatedly multiply the weight matrix between hidden states.}  
    \begin{itemize}
        \item Gradient explosion: $\left(\bm{W}_{hh}^\top \right)^{T-i}$ will become 
        very large when some eigenvalues of $\bm{W}_{hh}$ have absolute values greater than one, which makes $\frac{\partial L}{\partial \bm{h}_t}$ very large.
        \item Gradient vanishing: $\left(\bm{W}_{hh}^\top \right)^{T-i}$ will become very small when some eigenvalues of $\bm{W}_{hh}$ have absolute values less than one, which makes $\frac{\partial L}{\partial \bm{h}_t}$ very small.
        \item How to mitigate gradient explosion/vanishing: 1) gradient clipping, i.e., projecting the gradients back to a $L_2$ norm ball. 
        The radius of the ball can be decided from historical epochs.
        2) detach the gradients after a fixed number of time steps.
    \end{itemize}
    
\subsubsection{GRU}
    Gated Recurrent Units (GRU) are designed to 1) capture long- and short-term dependencies and 2) mitigate gradient explosion/vanishing.
    \begin{itemize}
        \item Reset gates: used to reset the hidden states in order to capture short-term dependencies by forgetting long-term dependencies.
        \item Update gates: used to maintain long-term dependencies by deciding whether to update the hidden states; in the extreme case, the hidden states can keep unchanged throughout the whole sequence.
    \end{itemize}
    
\subsubsection{LSTM}
    LSTM was designed to solve similar issues as that of GRU.
    There are three gates in a LSTM: input gates, output gates and forget gates. 
    There is also a neuron to store information called a memory cell.
    Check out this tutorial: \url{https://d2l.ai/chapter_recurrent-modern/lstm.html}.
    
    
\subsection{Deepwalk and Node2vec}
    \subsubsection{Negative Sampling}
    Deepwalk and Node2vec are unsupervised methods to learn node embedding.
    Suppose we have a graph $G=(V, E)$ with $\ndata$ nodes.
    We would like to learn an embedding $\bm{z}_i \in \R^\nfeat$ for each node $i$, such that $\bm{z}^{\top}_i \bm{z}_j$ is large when $i$ and $j$ are connected.
    Let $f(i): V \rightarrow \R^\nfeat$ be the function mapping a node to its embedding.
    We represent the neighborhood (not necessarily 1-hop neighbors) of a node $i$ by $\mathcal{N}_{s}(i)$, where $s$ is some strategy that we use to generate the neighborhood (e.g., random walk).
    Deepwalk and Node2vec solve the following maximum likelihood estimation problem:
        \begin{equation}
            \max_{f} \left[ \sum_{i=1}^{\ndata}{ \log p\left(\mathcal{N}_s(i) | f(i) \right)} \approx \sum_{i=1}^{\ndata}{ \sum_{j \in \mathcal{N}_s(i)}^{}{\log p(j | f(i))} } \right],
        \end{equation}
    where the approximation utilizes the assumption of conditional independence.
    The probability density function $p(j | f(i))$ is usually parameterized by a softmax:
        \begin{equation}
            p(j | f(i)) = \frac{\EXP{\bm{z}_j^\top \bm{z}_i}}{\sum_{k\in V}^{}{\EXP{\bm{z}_k^\top \bm{z}_i}}}.
        \end{equation}
    Intuitively, if node $j$ frequently occurs in random walks started from $i$, the two nodes $i$ and $j$ should be similar in some sense.
    Evaluating the softmax is very expensive due to the summation in the denominator.
    One solution is to utilize the idea of \emph{negative sampling} to do approximation:
        \begin{equation}
            \log \frac{\EXP{\bm{z}_j^\top \bm{z}_i}}{\sum_{k\in V}^{}{\EXP{\bm{z}_k^\top \bm{z}_i}}} \approx \log\left( \sigma(\bm{z}_j^\top \bm{z}_i) \right)  - \sum_{k\in \mathcal{M}}^{}{\log\left( \sigma(\bm{z}_k^\top \bm{z}_i) \right)},
        \end{equation}
    where $\sigma(\cdot)$ is the sigmoid function.
    The set $\mathcal{M}$ is a subset of nodes with small cardinality, i.e., $\SetCard{\mathcal{M}} \ll \SetCard{V}$.
    Typically, the set $\mathcal{M}$ should contain ``negative samples"; in the case of Deepwalk and Node2vec, the negative samples are those nodes that do not belong to any random walk starting from $i$; In practice, the nodes in $\mathcal{M}$ are sampled with probability proportional to the nodes' degrees.\footnote{The definition of negative samples depends on specific application scenarios.}
    
    
    
    \subsection{GNN}
        Two key operations in a GNN are:
        \begin{itemize}
            \item Message passing: a node $u$ generates some messages from its embedding, i.e., $\bm{m}_u^{(l)} = \text{MSG}^{(l)}(\bm{h}_u^{(l-1)})$. The operation $\text{MSG}^{(l)}$ can be as simple as some matrix multiplication, e.g., $\bm{W}^{(l)} \cdot \bm{h}^{(l-1)}_u$.
            \item Aggregation: a node $v$ generates its embedding at layer $l$ by aggregating the messages from its neighbors, i.e., $\bm{h}_v^{(l)} = \text{AGG}^{(l)}\left( \Set{\bm{m}_u^{(l)} }{u \in \mathcal{N}_v} \right)$.
        \end{itemize}
        
        \subsubsection{Over-Smoothing}
        The embedding of a node depends on a concept called \emph{receptive field}, which is the set of nodes where messages are aggregated (e.g., the $k$-hop neighborhood).
        Intuitively, over-smoothing means that the receptive fields of each node are highly overlapped with each other, such that their embeddings are converging to the same values.
        Over-smoothing can result from stacking too many layers of GNN.
        Some solutions include:
            \begin{itemize}
                \item Limit the number of layers; a good strategy to estimate the number of layers needed is to compute the diameter of the graph.
                \item Increase the expressive power, e.g., adding some MLP layers before/after the GNN, or resorting to some attention mechanisms.
                \item Add skip connections.
            \end{itemize}
        
        \subsubsection{Scaling up GNN}
            A key idea to scale the training of a GNN is to use mini-batch SGD.
            However, the tricky part is how do we assemble the mini-batch.
            
                \paragraph{Neighborhood Sampling.} The embedding of a node only depends on its $k$-hop neighborhood.
                Thus, we let a mini-batch include $m$ nodes as well as each node's $k$ hop neighbors.
                For a node $i$, the computation of its embedding is to expand a computational graph on its $k$-hop neighborhood; the neighborhood can be obtained by random-walk based sampling approach (e.g., PageRank).
                After obtaining the embedding, we  compute the loss on this node; the loss is usually an unsupervised likelihood function approximated by negative sampling.
                \paragraph{Cluster-GCN.} The basic idea is similar to that of \emph{neighborhood sampling}.
                we sample a small number of node groups (e.g., generated by some community detection algorithms) and construct their induced subgraph; conceptually, this induced subgraph plays the role of a ``mini-batch''.
                We then compute the loss on this  subgraph and update the weight matrices. 

            
\subsection{Dropout}
    Intuitively, we zero out some neurons during training with probability $p$.
    However, we need to adjust the outputs of the remaining neurons such that their expectations do not change.
    For any neuron $h$, the adjusted output is as follows:
        \begin{equation}
            h' = \begin{cases}
                    0, & \text{with prob. $p$} \\
                    \frac{h}{1-p}, & \text{with prob. $1-p$}.
                 \end{cases}
        \end{equation}
    As a result, the expected output is $\mathbbm{E}[h'] = h$, which is the same as if there were no dropout.
    When testing we do not zero out any neuron and the forward pass is the same as usual.
    
\subsection{Batch Normalization (BN)}
    BN is used to center and re-scale the inputs to each layer based on statistics computed from mini-batches, which may speed-up the convergence.
    Formally, BN is the following operations:
        \begin{equation}
            \text{BN}(\bm{x}) = \bm{\gamma} \odot \frac{\bm{x} - \hat{\bm{\mu}} }{\hat{\bm{\sigma}}} + \bm{\beta},
        \end{equation}
    where $\hat{\bm{\mu}}$ and $\hat{\bm{\sigma}}$ are sample mean and standard deviation estimated from the mini-batch; $\bm{\gamma}$ and $\bm{\beta}$ are learnable weight parameters.
    Notice that during training the mean and standard deviation of BN are computed from mini-batches, however, those statistics are computed from the entire dataset when testing.
        
    