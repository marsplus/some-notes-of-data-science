
\subsection{Some Fundamentals}
    Given a matrix $\bm{A} \in \R^{m \times n}$, the null space contains all solutions $\bm{x}$ to $\bm{A}\bm{x} = \bm{0}$.
    The reduced row echelon form of $\bm{A}$ is $\bm{R}$.
    Some important facts:
    \begin{itemize}
        \item \text{The number of pivots of $\bm{A}$} = \text{the number of nonzero rows in $\bm{R}$} = \text{$\Rank(\bm{A})$}.
        \item If column $j$ of $\bm{R}$ is free (i.e., no pivot), there is a special solution of $\bm{A}\bm{x}=\bm{0}$ with $x_j=1$.
        The number of free columns (i.e., the columns without pivots) of $\bm{R}$ is $n - \Rank(\bm{A})$.
        \item Every matrix with $m < n$ (i.e., a short and wide matrix) has nonzero solutions to $\bm{A}\bm{x} = \bm{0}$.
        \item The rank $\Rank(\bm{A})$ is the dimensions of both the row space and the column space. The dimension of the null space is $n - \Rank(\bm{A})$.
        \item The solutions to $\bm{A}\bm{x} = \bm{b}$ (if any) consists of two parts: one particular solution $\bm{x}_p$ where $\bm{A}\bm{x}_p = \bm{b}$ and several solutions $\bm{x}_n$ of $\bm{A} \bm{x} = \bm{0}$ (one for each free variable).
        \item In summary, there are four possibilities for the linear system $\bm{A} \bm{x} = \bm{b}$:
            \begin{enumerate}
                \item $\Rank(\bm{A}) = m = n$: the matrix $\bm{A}$ is invertible and the system has one solution.
                \item $\Rank(\bm{A}) = m$ and $\Rank(\bm{A}) < n$: the matrix $\bm{A}$ is short and wide; the system has infinite solutions.
                \item $\Rank(\bm{A}) < m$ and $\Rank(\bm{A}) = n$: the matrix $\bm{A}$ is tall and thin; the system has zero or one solution.
                \item $\Rank(\bm{A}) < m$ and $\Rank(\bm{A}) < n$: the matrix $\bm{A}$ is not full rank; the system has zero or infinite solutions.
            \end{enumerate}
        \end{itemize}




\subsection{Commonly Used Matrix Differentiation}
    We use the \emph{numerator layout} convention when discussing matrix differentiation.
    Without specific note, all vectors are column vectors.
    The numerator layout has the following rules:
        \begin{enumerate}
            \item The derivative of a scalar w.r.t. a row vector is a column vector, i.e., $\frac{\partial L}{\partial \bm{x}^\top} \in \R^{\nfeat \times 1}$.
            \item The derivative of a scalar w.r.t. a column vector is a row vector, i.e., $\frac{\partial L}{\partial \bm{x} } \in \R^{1 \times \nfeat}$.
            \item The derivative of a column vector w.r.t. a scalar is a column vector, i.e., $\frac{\partial \bm{x}}{\partial y } \in \R^{ \nfeat \times 1}$.
            \item The derivative of a row vector w.r.t. a scalar is a row vector, i.e., $\frac{\partial \bm{x}^\top }{\partial y } \in \R^{ 1 \times \nfeat}$.
        \end{enumerate}
    The following are some commonly used differentiation operations:
        \begin{itemize}
            \item $\bm{y} = \bm{A} \bm{x} \implies \frac{\partial \bm{y}}{\partial \bm{x}} = \bm{A}$.
            \item $\bm{y} = \bm{A} \cdot  \bm{x}(\bm{z}) \implies \frac{\partial \bm{y}}{\partial \bm{z}} = \frac{\partial \bm{y}}{\partial \bm{x}} \frac{\partial \bm{x}}{\partial \bm{z}} = \bm{A} \frac{\partial \bm{x}}{\partial \bm{z}}$.
            \item $\alpha = \bm{y}^\top \bm{A} \bm{x} \implies \frac{\partial \alpha}{\partial \bm{x}} = \bm{y}^\top \bm{A}$ (this satisfies the 2nd rule above). For the special case $\alpha = \bm{x}^\top \bm{A} \bm{x}$, the derivative $\frac{\partial \alpha}{\partial \bm{x}} = \bm{x}^\top (\bm{A} + \bm{A}^\top)$. When $\bm{A}$ is symmetric, the derivative is $\frac{\partial \alpha}{\partial \bm{x}} = 2\bm{x}^\top \bm{A}$.
            \item $\alpha = \bm{y}^\top(\bm{z}) \cdot \bm{x}(\bm{z}) \implies \frac{\partial \alpha}{\partial \bm{z}} = \bm{x}^\top \frac{\partial \bm{y}}{\partial \bm{z}} + \bm{y}^\top \frac{\partial \bm{x}}{\partial \bm{z}}$.
            \item $\alpha = \bm{x}^\top(\bm{z}) \cdot \bm{x}(\bm{z}) \implies \frac{\partial \alpha}{\partial \bm{z}} = \bm{x}^\top \frac{\partial \bm{x}}{\partial \bm{z}} + \bm{x}^\top \frac{\partial \bm{x}}{\partial \bm{z}} = 2\bm{x}^\top \frac{\partial \bm{x}}{\partial \bm{z}}$.
        \end{itemize}
        Zico Kolter from CMU mentioned a very cool hack to compute matrix differentials. 
        Basically, we first assume everything is scalar (i.e., one-by-one matrices); then, we figure out what the dimensions should be; check out the video \url{shorturl.at/fHRW2}.
        
        
        
\subsection{Solving Linear Systems}
    Suppose we need to solve a linear system $\bm{A} \bm{x} = \bm{b}$, where $\bm{A}$ is a real square matrix.
    The standard way is as follows:
        \begin{itemize}
            \item LU decomposition: we first get the LU decomposition of $\bm{A}$, i.e., $\bm{A} = \bm{L} \bm{U}$, where $\bm{L}$ is a lower-triangular matrix and $\bm{U}$ a upper-triangular matrix.
            With the decomposition, the problem is equivalent to solve $\bm{A} \bm{x} = \bm{L} \bm{U} \bm{x}$ for $\bm{x}$.
            We first solve $\bm{L} \bm{y} = \bm{b}$ for $\bm{y}$; this is direct since $\bm{L}$ is a lower-triangular matrix.
            Then, we solve $\bm{U} \bm{x} = \bm{y}$; this is also direct as $\bm{U}$ is a upper-triangular matrix.
            \item If $\bm{A}$ is symmetric positive definite: in this case the LU decomposition simplifies to $\bm{A} = \bm{L} \bm{L}^\top$.
        \end{itemize}
        

