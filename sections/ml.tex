

In this section, a data matrix is represented by $\bm{X} \in \R^{\ndata \times \nfeat}$, with \ndata the number of data points and \nfeat the number of features. 
When the context is clear we sometimes use $\mathcal{S}$ to refer an unlabeled data. 

\subsection{Common Loss Functions}
In this section, we focus on two discrete distributions $p$ and $q$ , defined on the same set $\mathcal{X}$.
    \begin{itemize}
        \item KL divergence: $D_\text{KL}(p || q) = \sum_{i \in \mathcal{X}}^{}{p(i) \log \frac{p(i)}{q(i)}}$.
        \item Entropy:  $H(p) = -\sum_{i \in \mathcal{X}}^{}{p(i) \log p(i)}$, which captures the uncertainty of the underlying event set.
        \item Cross entropy: 
            \begin{equation*}
               H(p, q) = H(p) + D_\text{KL}(p || q) =  -\sum_{i \in \mathcal{X}}^{}{p(i) \log p(i)} + \sum_{i \in \mathcal{X}}^{}{p(i) \log \frac{p(i)}{q(i)}} =  -\sum_{i \in \mathcal{X}}^{}{p(i) \log q(i)}.
            \end{equation*}
            Minimizing cross entropy is equivalent to maximizing the likelihood of observed data.
            Thus, when we update the model's parameters to minimize cross entropy, we are actually updating the model so as to maximize the data likelihood.
        \item Information gain from $q$ to $p$ (or the reduction of uncertainty): $H(q) - H(p)$.
        \item Mutual information (in discrete spaces): 
            \begin{equation*}
                I(X; Y) = D_\text{KL}\left( P_{X, Y} \, || \, P_X \cdot P_Y \right).
            \end{equation*}
        MI is the KL divergence between the joint distribution (of $X$ and $Y$) and the product of their marginal distributions.
        It measures the extent to which knowledge of one quantity reduces uncertainty about the other.
        \item Mean squared error: $\frac{1}{\ndata}\sum_{i=1}^{\ndata}{(y_i - \hat{y}_i)^2}$.
        \item Upper-bounds on 0-1 loss:
            \begin{itemize}
                \item Exponential loss: $e^{-y \cdot f(\bm{x})}$.
                \item Hinge loss: $\max\SET{1 - y \cdot f(\bm{x}), 0}$.
                \item Logistic loss: $\log_2\left( 1 + e^{-y \cdot f(\bm{x})}\right)$. 
            \end{itemize}
    \end{itemize}
    

\subsection{Common Data Preprocessing}
    \begin{itemize}
        \item Standardization: subtract the mean and divided by the standard deviation. The processed features have zero mean and unit variance.
        \item Whitening: do a PCA and then project the data to the principal components, which removes correlation between features. 
    \end{itemize}
    


\subsection{Bias-Complexity Tradeoff}
One fundamental reason that the so-called ``bias-complexity'' tradeoff arises is the No-Free-Lunch theorem; see Theorem 5.1 at \cite{shalev2014understanding} for the formal statement and proof.
Intuitively, the No-Free-Lunch theorem shows that we can't find a predictor that is good at every learning task. 
As a result, we need to encode some prior knowledge about the underlying task (i.e., the distribution over the input domain $\mathcal{X} \times \mathcal{Y}$) into the learning process.
One way to encode the knowledge is choosing a particular hypothesis set $\mathcal{H}$ to work with, e.g., $\mathcal{H}$ can be the set of linear predictors.
However, encoding our prior is also the root of bias.

The generalization error of an ERM predictor $h \in \mathcal{H}$ is denoted by $L(h)$, which (informally) consists of two terms:
    \begin{equation}
        L(h) = L_\text{bias} + L_\text{est}.
    \end{equation}
\begin{itemize}
    \item The first term measures the bias of the hypothesis set $\mathcal{H}$: $L_\text{bias} = \argmin_{h \in \mathcal{H}} L(h)$.
          Intuitively, if $\mathcal{H}$ is some complicated hypothesis set, e.g., deep neural networks, the bias will be very small; however, a very small bias would not always be good, as the complicated hypothesis set is the root of overfitting.
          
    \item Informally, the second term decomposes into
        \begin{equation}
            L_\text{est} \approx \underbrace{L_{\mathcal{S}}(h)}_{\text{training error on $\mathcal{S}$}} + f(\text{model and sample complexity}).
        \end{equation}
    Thus, a complicated model with have high model complexity, which makes $f(\cdot)$ large.
    In addition, a complicated model tends to overfit to the training set; as a result, the training error $L_\mathcal{S}(h)$ will change a lot for different training data--- this refers to the ``variance'' in the famous Bias-Variance tradeoff.
    
    \item How the Bias-Variance tradeoff arises
        \begin{equation}
            \begin{aligned}
                    & \text{reduce bias} \implies \text{complicated $\mathcal{H}$} \implies \text{overfitting} \implies \text{large variance} \\
                    & \text{reduce variance} \implies \text{simple $\mathcal{H}$}   \implies \text{underfitting} \implies \text{large bias}.
            \end{aligned}
        \end{equation}
\end{itemize}

    \subsubsection{A Derivation of Bias-Variance Trade-off}
        Suppose we have a dataset $\mathcal{D}=\SET{(\bm{x}_1, y_1), \ldots, (\bm{x}_\ndata, y_\ndata)}$ with i.i.d. samples.
        Consider a regression setting with squared error loss.
        The following definitions will be useful later:
            \begin{itemize}
                \item Expected label given the features: 
                    \begin{equation}
                        \bar{y}(\bm{x}) = \mathbbm{E}_{y | \bm{x}}\left[ Y \right] = \int_{y}^{}{ y \cdot P(y | \bm{x}) \partial y}.
                    \end{equation}
                \item Expected test error given a hypothesis learned from $\mathcal{D}$:
                    \begin{equation}
                        \mathbbm{E}_{(\bm{x}, y) \sim P(\bm{x}, y)}\left[ (h_{\mathcal{D}}(\bm{x}) - y )^2 \right] = \int_{\bm{x}}^{}{ \int_{y}^{}{ (h_{\mathcal{D}}(\bm{x}) - y )^2 P(\bm{x}, y)   }     \partial \bm{x} \partial y}.
                    \end{equation}
                \item Expected classifier due to the randomness of sampling $\mathcal{D}$:
                    \begin{equation}
                        \bar{h}(\bm{x}) = \mathbbm{E}_{\mathcal{D} \sim P^n}\left[ h_{\mathcal{D}}(\bm{x}) \right] = \int_{\mathcal{D}}^{}{ h_{\mathcal{D}}(\bm{x})P(\mathcal{D}) \partial \mathcal{D}}.
                    \end{equation}
                \item Expected test error that we ultimately care about: 
                    \begin{equation}\label{eq:bias-var-gen}
                        \mathbbm{E}_{\bm{x}, y, \mathcal{D}}\left[ (h_{\mathcal{D}}(\bm{x}) - y )^2 \right] = \int_{\bm{x}}^{}{\int_{y}^{}{\int_{\mathcal{D}}^{}{        (h_{\mathcal{D}}(\bm{x}) - y )^2 P(\mathcal{D})P(\bm{x}, y) \partial \bm{x} \partial y \partial \mathcal{D}
                        }}}.
                    \end{equation}
            \end{itemize}
            
        Now, we expand \eqref{eq:bias-var-gen} as follows:
            \begin{equation}
                \begin{aligned}
                    \mathbbm{E}_{\bm{x}, y, \mathcal{D}}\left[ (h_{\mathcal{D}}(\bm{x}) - y )^2 \right] & = \mathbbm{E}_{\bm{x}, y, \mathcal{D}}\left[ \left( h_{\mathcal{D}}(\bm{x}) - \bar{h}(\bm{x}) + \bar{h}(\bm{x}) - y \right)^2 \right] \\
                    & = \mathbbm{E}_{\bm{x}, y, \mathcal{D}}\left[ (h_{\mathcal{D}}(\bm{x}) - \bar{h}(\bm{x}))^2 \right] + \mathbbm{E}_{\bm{x}, y, \mathcal{D}}\left[ 2(h_{\mathcal{D}}(\bm{x}) - \bar{h}(\bm{x}))(\bar{h}(\bm{x}) - y) \right] + \mathbbm{E}_{\bm{x}, y, \mathcal{D}}\left[ (\bar{h}(\bm{x}) - y)^2 \right],
                \end{aligned}
            \end{equation}
        where the middle term is zero becauses:
            \begin{equation}
                \begin{aligned}
                      \mathbbm{E}_{\bm{x}, y, \mathcal{D}}\left[ 2(h_{\mathcal{D}}(\bm{x}) - \bar{h}(\bm{x}))(\bar{h}(\bm{x}) - y) \right] & = 2\mathbbm{E}_{\bm{x}, y}\left[ \mathbbm{E}_{\mathcal{D}}\left[ (h_{\mathcal{D}}(\bm{x}) - \bar{h}(\bm{x}) ) \right](\bar{h}(\bm{x}) - y) \right]      \\
                      & = 2\mathbbm{E}_{\bm{x}, y}\left[  ( \mathbbm{E}_{\mathcal{D}}[h_{\mathcal{D}}(\bm{x})] - \bar{h}(\bm{x}) ) (\bar{h}(\bm{x}) - y) \right] \\
                      & = 2\mathbbm{E}_{\bm{x}, y}\left[  ( \bar{h}(\bm{x}) - \bar{h}(\bm{x}) ) (\bar{h}(\bm{x}) - y) \right] \\
                      & = 0.
                \end{aligned}
            \end{equation}
        The third term is expanded as follows:
            \begin{equation}
                \begin{aligned}
                \mathbbm{E}_{\bm{x}, y, \mathcal{D}}\left[ (\bar{h}(\bm{x}) - y)^2 \right] & = \mathbbm{E}_{\bm{x}, y }\left[ (\bar{h}(\bm{x}) - \bar{y}(\bm{x}) + \bar{y}(\bm{x}) - y)^2 \right] \\
                & = \mathbbm{E}_{\bm{x}, y }\left[ \left( \bar{h}(\bm{x}) - \bar{y}(\bm{x}) \right)^2 \right] + 2\mathbbm{E}_{\bm{x}, y }\left[ \left( \bar{h}(\bm{x}) - \bar{y}(\bm{x}) \right)\left( \bar{y}(\bm{x}) - y \right) \right] + \mathbbm{E}_{\bm{x}}\left[ \left( \bar{y}(\bm{x}) - y \right)^2 \right],
                \end{aligned}
            \end{equation}
        where the middle term is zero because:
            \begin{equation}
                \begin{aligned}
                        2\mathbbm{E}_{\bm{x}, y }\left[ \left( \bar{h}(\bm{x}) - \bar{y}(\bm{x}) \right)\left( \bar{y}(\bm{x}) - y \right) \right]    & = 2\mathbbm{E}_{\bm{x}}\left[ \left( \bar{h}(\bm{x}) - \bar{y}(\bm{x}) \right)    \mathbbm{E}_{y|\bm{x}}\left[\left( \bar{y}(\bm{x}) - y \right) \right] \right] \\
                        & = 2\mathbbm{E}_{\bm{x}}\left[ \left( \bar{h}(\bm{x}) - \bar{y}(\bm{x}) \right) \left( \bar{y}(\bm{x}) - \mathbbm{E}_{y|\bm{x}}[y] \right) \right] \\
                        & = 2\mathbbm{E}_{\bm{x}}\left[ \left( \bar{h}(\bm{x}) - \bar{y}(\bm{x}) \right) \left( \bar{y}(\bm{x}) - \bar{y}(\bm{x}) \right) \right] \\
                        & = 0.
                \end{aligned}
            \end{equation}
        Finally, the generalization error that we care about is:
            \begin{equation}
                \mathbbm{E}_{\bm{x}, y, \mathcal{D}}\left[ (h_{\mathcal{D}}(\bm{x}) - y )^2 \right] = \underbrace{\mathbbm{E}_{\bm{x}, \mathcal{D}}\left[ (h_{\mathcal{D}}(\bm{x}) - \bar{h}(\bm{x}))^2 \right]}_{\text{variance}} + \underbrace{\mathbbm{E}_{\bm{x} }\left[ \left( \bar{h}(\bm{x}) - \bar{y}(\bm{x}) \right)^2 \right]}_{\text{bias}^2} + \underbrace{\mathbbm{E}_{\bm{x}}\left[ \left( \bar{y}(\bm{x}) - y \right)^2 \right]}_{\text{noise}}.
            \end{equation}
        From the above derivation we have the following conclusions:
            \begin{itemize}
                \item Variance: capturing how much the hypothesis changes w.r.t. different training data, which quantifies overfitting, i.e., the hypothesis is too specialized to a particular training data.
                \item Bias: capturing the inherent difference between the hypothesis and the expected label even with infinite data.
            \end{itemize}
        Check out this lecture: \url{https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote12.html}.
        
        \subsubsection{An Example of Bias-Variance for Linear Regression Models }
        This is based on Chapter 7.3 at~\cite{hastie2009elements}.
        Consider a linear regression model with standard gaussian noise $\epsilon$:
            \begin{equation}
                Y = f(X) + \epsilon.
            \end{equation}
        The expected prediction error of a regression fit $\hat{f}$ at $x_0$ is:
            \begin{equation}
                \begin{aligned}
                    \ell(x_0) & = \mathbbm{E}[(f(x_0) - \hat{f}(x_0))^2] \\
                    & = \text{noise} + \underbrace{\mathbbm{E}\left[\left(\hat{f}(x_0) - \mathbbm{E}\hat{f}(x_0) \right)^2 \right]}_{\text{variance of $\hat{f}$}} + \underbrace{\left( \mathbbm{E}\hat{f}(x_0) - f(x_0) \right)^2}_{\text{bias}^2}
                \end{aligned}
            \end{equation}
        
        


\subsection{Cross-Validation}
    We usually use $K$-fold cross-validation to choose the hyper-parameters of the model, in order to avoid overfitting.
    To select the value of $K$, we can generate a curve where the size of each fold is plotted against the prediction accuracy. 
    The curve should have some diminishing return property, that is, at some point increasing the size of a fold (or equivalently decreasing $K$) does not benefit prediction; we then settle to the point; see~\cite{hastie2009elements}(p.243) for more details.



\subsection{Precision, Recall, and some Other Measures}
    The quizzes here are instructive: \url{shorturl.at/cwALX}.
    \begin{itemize}
        \item Precision: $\frac{TP}{TP + FP}$; what proportion of identified positives was actually positives?
        \item Top-k Precision: this is usually used in fraud/anomaly detection. Specifically, the predictive model gives a list of $k$ suspicious entries.
        Then, $\text{top-k precision} = \frac{\text{TP out of the $k$ entries}}{k}$.
        \item Recall: $\frac{TP}{TP + FN}$; what proportion of true positives that was correctly identified?
        \item F1 score: $\left( \frac{\text{Precision}^{-1} + \text{Recall}^{-1}}{2} \right)^{-1}$; a measure to consider both precision and recall.
        \item ROC curve: evaluating a binary classification model at different thresholds and plotting the false positive rates (as x-axis) against the true positive rates (as y-axis).
        \item AUC: the area under the ROC curve. Intuitively, AUC gives us the probability that the model ranks a random positive example higher than a random negative example.
            \begin{enumerate}
                \item AUC is scale-invariant: it does not depend on the actual magnitudes of the output; in other words, only the ranking of the outputs matter.
                \item AUC measures the quality of the prediction irrespective of what threshold is chosen: this may not be desirable if we have different costs for false positives and false negative (e.g., spam detection).
            \end{enumerate}
    \end{itemize}
    


\subsection{MLE and MAP}
    \begin{itemize}
        \item The optimization problem of MLE: $\bm{\theta}^\ast \in  \argmin_{\bm{\theta}} p(\mathcal{S} | \bm{\theta})$, where $p(\mathcal{S}| \bm{\theta})$ is the likelihood of data $\mathcal{S}$.
            \begin{enumerate}
                \item Bernoulli distribution (coin flip): suppose we flip a coin $\ndata$ times, i.e., $X_1, \ldots, X_\ndata$ where $X_i=1$ (resp. $X_i=0$) indicating head (resp. tail).
                The probability of getting a head is $p$.
                The log-likelihood function is:
                    \begin{equation}
                        \begin{aligned}
                            \log L & = \log \prod_{i=1}^{\ndata}{
                            p^{X_i} (1-p)^{1-X_i} } \\
                            & = \sum_{i=1}^{\ndata}{X_i \log p + (1-X_i) \log(1-p)} \\
                            & \quad \quad \text{let $s = \sum_{i=1}^{\ndata}{X_i}$} \\
                            & = \log p \cdot s + \log (1-p) (n-s).
                        \end{aligned}
                    \end{equation}
                Let $\frac{\partial \log L}{\partial p} =0$ we have $p = s / \ndata$.
                \item Uniform distribution: suppose $X_1, \ldots, X_\ndata$ are sampled from a uniform distribution $[0, \theta]$.
                We would like to estimate $\theta$.
                Recall that the PDF of a uniform distribution is:
                    \begin{equation}
                        f(x; \theta) = \begin{cases}
                            \frac{1}{\theta}, & 0 \le x \le \theta \\
                            0, & o.w.
                        \end{cases}
                    \end{equation}   
                Let $X^\ast = \max \SET{X_1, \ldots, X_\ndata}$.
                If $X^\ast > \theta$, we know that the likelihood function $L = 0$; otherwise, the likelihood function is $L = \left( \frac{1}{\theta}\right)^\ndata$.
                The MLE estimate of $\theta$ is $X^\ast$.
                Notice that $\theta > X^\ast$ cannot be true, as $L$ is decreasing in $\theta$; also, we cannot have $\theta < X^\ast$, which makes $L=0$. 
            \end{enumerate}
        \item The optimization problem of MAP: $\bm{\theta}^\ast \in  \argmin_{\bm{\theta}} p(\bm{\theta} | \mathcal{S})$, where $p(\bm{\theta} | \mathcal{S}) \propto p(\bm{\theta})p(\mathcal{S} | \bm{\theta})$.
        \item If we have some prior knowledge about the parameter $\bm{\theta}$, then MAP is a good choice; otherwise, go ahead and use MLE. 
    \end{itemize}
        
    

\subsection{Overfitting}
    Intuitively, overfitting means that a model has low error on training set but poor performance when testing on unseen data.
    Cross-validation is a good tool to assess a model's generalization ability and detect overfitting.
    The following are commonly used methods to combat overfitting:
        \begin{itemize}
            \item Get more data.
            \item Data augmentation.
            \item Regularization $ \rightarrow $ reduce model complexity $ \rightarrow $ reduce variance $ \rightarrow $ reduce overfitting.
            \item Ensemble methods: random forests, bagging, etc.
            \item Feature selection $ \rightarrow $ reduce model complexity.
        \end{itemize}



\subsection{Singular Value Decomposition (SVD)}
    \begin{itemize}
        \item Given a data matrix $\bm{X} \in \R^{\ndata \times \nfeat}$, where \ndata is the number of data points and \nfeat the number of features.
        The SVD is: $\bm{X} = \bm{U} \bm{D} \bm{V}^\top$; $\bm{U} \in \R^{\ndata \times \nfeat}$ and the columns of $\bm{U}$ span the column space of $\bm{X}$; $\bm{V} \in \R^{\nfeat \times \nfeat}$ and the columns of $\bm{V}$ span the row space of $\bm{X}$.
        Both $\bm{U}$ and $\bm{V}$ are orthonormal matrices. 
        $\bm{D}$ is a diagonal matrix with some diagonal entries possibly zeros.
    \end{itemize}
    
    
\subsection{Principal Component Analysis (PCA)}
    \begin{itemize}
        \item Given a mean-centered data matrix $\bm{X} \in \R^{\ndata \times \nfeat}$, i.e., the sum of the rows of $\bm{X}$ is $\bm{0}$; in other words, each feature has zero mean.
        The correlation between features is captured by the unnormalized sample covariance matrix $\bm{X}^\top \bm{X}$, i.e., $X_{ij}$ quantifies the correlation between features $i$ and $j$.
        PCA is to do en  eigen-decomposition of $\bm{X}^\top \bm{X}$ as follows\footnote{A real symmetric matrix is always diagonalizable; see \url{http://maecourses.ucsd.edu/~mdeolive/mae280a/lecture11.pdf}.}:
            \begin{equation}
                \bm{X}^\top \bm{X} = \left( \bm{U}\bm{D}\bm{V}^\top \right)^\top \left( \bm{U}\bm{D}\bm{V}^\top \right) = \bm{V} \bm{D}^2 \bm{V}^\top,
            \end{equation}
        where $\bm{V}^{-1} = \bm{V}^\top$. 
        The diagonal entries of $\bm{D}^2$ are ranked in descending order.
        As $\bm{X}^\top \bm{X}$ is a real symmetric matrix, it can be diagonalized by an orthogonal matrix.
        \item The first principal component is $\bm{z}_1 = \bm{X} \bm{v}_1$, i.e., the projection of the data matrix to the first column vector of $\bm{V}$; $\bm{z}_1$ is also the direction with the maximum sample variance.
        
        \item Notice that $\bm{X} \bm{v}_1 = d_1 \bm{u}_1$ (since $\bm{X} \bm{v}_1 = \bm{U} \bm{D} \bm{V}^\top \bm{v}_1$...), where $d_1$ is the first diagonal entry of $\bm{D}$ and $\bm{u}_1$ the first column vector of $\bm{U}$; thus, the first principal component is parallel to $\bm{u}_1$.
    \end{itemize}


\subsection{Regressions}
    One thing worth mentioning is that many regression problems require computing matrix inverse.
    An efficient and numerically stable way to compute the inverse $\bm{X}^{-1}$ is to solve the linear system $\bm{X} \bm{y} = \bm{I}$ for $\bm{y}$, e.g., by using \texttt{Numpy}.\footnote{Check out: \url{https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html}}
    
    \subsubsection{Linear Regression}\label{ML:linear-regression}
        \begin{itemize}
            \item The optimization problem: $\hat{\bm{\beta}} \in \argmin_{\bm{\beta}} \norm{\bm{X} \bm{\beta} - \bm{y}}_2^2$.
            \item The objective function: $L = \bm{\beta}^\top \bm{X}^\top \bm{X} \bm{\beta} - 2 \bm{\beta}^\top \bm{X}^\top \bm{y} - \bm{y}^\top \bm{y}$.
            \item The objective function is convex w.r.t. $\beta$. Thus, the first-order optimality condition is: 
                \begin{equation}
                    \frac{\partial L}{\partial \bm{\beta}^\top} =  2 \bm{X}^\top \bm{X} \bm{\beta} - 2\bm{X}^\top \bm{y} = \bm{0}.
                \end{equation}
            Setting the derivative to zero we have: $\hat{\bm{\beta}} = (\bm{X}^\top \bm{X})^{-1} \bm{X}^\top \bm{y}$.
            \item Further discussion: Chapter 3.2 of~\cite{hastie2009elements}.
        \end{itemize}
    
    \subsubsection{Ridge Regression}
        \begin{itemize}
            \item The optimization problem: $\hat{\bm{\beta}} \in \argmin_{\bm{\beta}} \norm{\bm{X} \bm{\beta} - \bm{y}}_2^2 + \lambda\norm{\bm{\beta}}_2^2$.
            \item The objective function: $L = \bm{\beta}^\top \bm{X}^\top \bm{X} \bm{\beta} - 2 \bm{\beta}^\top \bm{X}^\top \bm{y} - \bm{y}^\top \bm{y} + \lambda \bm{\beta}^\top \bm{\beta}$.
            \item The objective function is convex w.r.t. $\bm{\beta}$. So setting the first-order derivative to zero leads to: $\hat{\bm{\beta}} = (\bm{X}^\top \bm{X} + \lambda \bm{I})^{-1} \bm{X}^\top \bm{y}$.
            \item NOTICE: when solving a ridge regression, we need to standardize (e.g., divided each column with the corresponding largest value)  and center the input $\bm{X}$. In addition, the intercept term is usually left out when solving the optimization problem and estimated separately by $\beta_0 = \frac{1}{\ndata}\sum_{i=1}^{\ndata}{y_i}$.
            \item Intuitively, the solution $\hat{\bm{\beta}}$ of a ridge regression shrinks the solution of a vanilla linear regression.
            \item The $L_2$ regularization has two benefits: 1) control model complexity (i.e., low variance); 2) improve robustness of $\bm{\beta}$ to the data. Further discussion: Chapter 3.4 of~\cite{hastie2009elements}.
        \end{itemize}
        
    \subsubsection{Lasso Regression}
        \begin{itemize}
            \item The optimization problem: $\hat{\bm{\beta}} \in \argmin_{\bm{\beta}} \frac{1}{2}\norm{\bm{X} \bm{\beta} - \bm{y}}_2^2 + \lambda \sum_{j=1}^{\nfeat}{\Abs{\beta_j}}$.
            \item The general idea to solve Lasso is utilizing the proximal gradient descent.
            See this note for deatails: \url{https://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf}.
            \item A commonly used algorithm to solve Lasso is \emph{Iterative Soft-Thresholding Algorithm} (ISTA). 
            \item ISTA algorithm is as follows:
                \begin{enumerate}
                    \item Let $g(\bm{x}) = \frac{1}{2}\norm{\bm{X} \bm{\beta} - \bm{y}}_2^2$. Then $\frac{d g}{d \bm{\beta}^\top} = -\bm{X}^\top(\bm{y} - \bm{X}\bm{\beta})$.
                    \item The iterate at step $t$ is:
                        \begin{equation}
                            \bm{\beta}^t = S_{\eta \lambda}\left( 
                                \bm{\beta}^{t-1} - \eta \frac{d g}{d \bm{\beta}^\top} 
                            \right) = S_{\eta \lambda}\left( 
                                \bm{\beta}^{t-1} + \eta \bm{X}^\top(\bm{y} - \bm{X}\bm{\beta}) 
                            \right),
                        \end{equation}
                    where $\eta$ is the learning rate; $S_{\eta \lambda}(\cdot)$ is the soft-thresholding operator with threshold $\eta \lambda$. 
                \end{enumerate}
        \end{itemize}
    
    \subsubsection{$L_1$ Regularization v.s. $L_2$ Regularization}
        \begin{itemize}
            \item $L_2$ regularization tends to shrink the learned parameters while $L_1$ squeezes some parameters to exactly zeros.
            \item $L_1$ makes the optimization a bit more challenging, as the proximal gradient descent needs to be applied.
            \item From robust optimization perspective, $L_1$ and $L_2$ correspond to two ways of constructing uncertainty sets. 
        \end{itemize}
    
\subsection{Classification}
        \subsection{Multi-Classification}
            Consider a $K$ classification problem.
            The following techniques can be applied to handle multi-classification problems:
            \begin{itemize}
                \item Softmax: this extends the sigmoid function used in binary classification problems.
                \item One versus one: train  $\binom{K}{2}$ classifiers, each for discriminating a pair of classes.
                Given a test instance, run it through the $\binom{K}{2}$ classifiers; the class with the most votes is the predicted class.
                \item One versus all: train $K$ classifiers, each for discriminating whether the given instance belongs to the associated class.
                Given a test instance, the class with the largest confidence (e.g., the distance to the separating hyperplane) is the predicted class.
            \end{itemize}
            
        \subsubsection{Bayes Optimal Classifier}
            In binary classification setting, the Bayes Optimal Classifier is defined as:
                \begin{equation}
                    h(\bm{x}) = \begin{cases} 1, & p(y=1 | \bm{x}) \ge 0.5 \\
                                              0, & p(y=1 | \bm{x}) < 0.5.
                    \end{cases}
                \end{equation}      
            It is optimal because no classifier can have lower misclassification rate than $h(\bm{x})$.
        \subsubsection{Logistic Regression (LR)}
            \begin{itemize}
                \item The general formulation of LR for $K$ classes is as follows (with the bias term separately estimated):
                    \begin{equation}
                        \begin{aligned}
                            & P(y=k | \bm{x}) = \frac{ \EXP{\beta_{k0} + \bm{\beta}_k^\top \bm{x} }}{1 + \sum_{l=1}^{K-1}{\EXP{\beta_{l0} + \bm{\beta}_l^\top \bm{x} } }} \\
                            & P(y=K | \bm{x}) = \frac{1}{1 + \sum_{l=1}^{K-1}{\EXP{\beta_{l0} + \bm{\beta}_l^\top \bm{x} } }}.
                        \end{aligned}
                    \end{equation}
                \item In the case of $K=2$ classes, the likelihood function for \ndata samples is:
                    \begin{equation}
                        L = \prod_{i=1}^{\ndata}{p(y=1 | \bm{x}_i)^{y_i} \left( 1-p(y=0 | \bm{x}_i) \right)^{1-y_i} },
                    \end{equation}
                and the corresponding log-likelihood function is:
                    \begin{equation}
                        \begin{aligned}
                        \log L & = \sum_{i=1}^{\ndata}{\left( y_i \log\frac{\EXP{\bm{\beta}^\top\bm{x}_i}}{1 + \EXP{\bm{\beta}^\top \bm{x}_i }}  + (1-y_i)\log\frac{1}{1+\EXP{\bm{\beta}^\top \bm{x}_i}} \right) }  \\
                              & = \sum_{i=1}^{\ndata}{\left( y_i \bm{\beta}^\top \bm{x}_i -\log(1 + \EXP{\bm{\beta}^\top \bm{x}_i} \right)}.
                        \end{aligned}
                    \end{equation}
                \item Computing the derivative of $\log L$ w.r.t. $\bm{\beta}$ leads to: 
                $\frac{\partial \log L}{\partial \bm{\beta}^\top} = \sum_{i=1}^{\ndata}{\left( y_i - p(y=1|\bm{x}_i) \right) \bm{x}_i }$.
                \item We usually maximize the log-likelihood by second-order methods, e.g., the Newton's method, as follows:
                    \begin{equation}
                        \bm{\beta}^t = \bm{\beta}^{t-1} - \eta \left[ \frac{\partial^2 \log L}{\partial \bm{\beta}^\top \partial \bm{\beta}} \right]^{-1} \frac{\partial \log L}{\partial \bm{\beta}^\top}.
                    \end{equation}
                \item The second-order derivative is computed as follows:
                    \begin{equation}
                        \begin{aligned}
                            \frac{\partial^2 \log L}{\partial \bm{\beta}^\top \partial \bm{\beta}} & = -\sum_{i=1}^{\ndata}{\bm{x}_i \frac{\partial p(y=1 | \bm{x}_i)}{\partial \bm{\beta}}   }  \\
                            & = - \sum_{i=1}^{\ndata}{\bm{x}_i \left( \frac{\partial [1 - \frac{1}{1+\EXP{\bm{\beta}^\top\bm{x}_i}}] }{\partial \bm{\beta}} \right)} \\
                            & = - \sum_{i=1}^{\ndata}{\bm{x}_i \left( \frac{}{} \bm{x}^{\top}_i \frac{\EXP{\bm{\beta}^\top \bm{x}_i}}{\left( 1 + \EXP{\bm{\beta}^\top \bm{x}_i}\right)^2} \right)} \\
                            & = -\sum_{i=1}^{\ndata}{\left( \bm{x}_i \bm{x}_i^\top p(y=1 | \bm{x}_i) \left( 1 - p(y=1 | \bm{x}_i)\right) \right)}.
                        \end{aligned}
                    \end{equation}
                \item Define $\bm{W}$ as a diagonal matrix with the $i$-th diagonal entry being $p(y=1| \bm{x}_i)(1 - p(y=1 | \bm{x}_i))$ and $\bm{p}$ a vector with the $i$-th entry being $p(y=1|\bm{x}_i)$.
                The first- and second-order derivatives are as follows:
                    \begin{equation}
                        \begin{aligned}
                            \frac{\partial \log L}{\partial \bm{\beta}^\top} & = \bm{X}^\top (\bm{y} - \bm{p}) \\
                            \frac{\partial^2 \log L}{\partial \bm{\beta}^\top \partial \bm{\beta}} & = -\bm{X}^\top \bm{W} \bm{X}.
                        \end{aligned}
                    \end{equation}  
                \item Using the above matrix notations to re-write the iterative updating rule:
                    \begin{equation}
                        \begin{aligned}
                            \bm{\beta}^t & = \bm{\beta}^{t-1} + \left(\bm{X}^\top \bm{W} \bm{X}\right)^{-1} \bm{X}^\top (\bm{y} - \bm{p}) \\
                                         & = \left(\bm{X}^\top \bm{W} \bm{X}\right)^{-1}\bm{X}^\top \bm{W} \bm{X} \bm{\beta}^{t-1} + \left(\bm{X}^\top \bm{W} \bm{X}\right)^{-1} \bm{X}^\top \bm{W} \bm{W}^{-1} (\bm{y} - \bm{p}) \\
                                         & = \left(\bm{X}^\top \bm{W} \bm{X}\right)^{-1}\bm{X}^\top \bm{W} \left( \underbrace{\bm{X}\bm{\beta}^{t-1} + \bm{W}^{-1}(\bm{y} - \bm{p})}_{:=\bm{z}} \right) \\
                                         & = \left(\bm{X}^\top \bm{W} \bm{X}\right)^{-1}\bm{X}^\top \bm{W} \bm{z}.
                        \end{aligned}
                    \end{equation}
                The last step is called \emph{Iteratively Reweighted Least Square}.
                Compare it with the update rule of linear regression, i.e., $\left( \bm{X}^\top \bm{X}\right)^{-1} \bm{X}^\top \bm{y}$.
                \item The Fisher information matrix is defined as $-\mathbbm{E}_{\bm{\beta}}[ \frac{\partial^2 \log f(\bm{x}; \bm{\beta})}{\partial \bm{\beta}^\top \partial \bm{\beta}} ]$, where the randomness comes from the sampling distribution of $\bm{\beta}$.
                From the above analysis we have that the Fisher information is $\bm{X}^\top \bm{W} \bm{X}$.
                The sampling distribution of $\bm{\beta}$ is \emph{approximately} Normal, i.e., $\bm{\beta} \sim \mathcal{N}(\bm{\beta}^\ast, \left(\bm{X}^\top \bm{W} \bm{X}\right)^{-1})$.
                \item In the case of binary classification, the log-odds satisfies:
                    \begin{equation}
                        \log \frac{P(y=1 | \bm{x})}{P(y=0 | \bm{x})}  = \beta_0 + \beta_1 x_1 +, \ldots, + \beta_\nfeat x_\nfeat.
                    \end{equation}
                Thus, the $i$-th coefficient $\beta_i$ represents the amount of increase or decrease of the log-odds when $x_i$ increases one unit.
            \end{itemize}
            
    \subsubsection{Naive Bayes (NB)}
        NB is based on Bayes Theorem: 
            \begin{equation}
                \begin{aligned}
                    p(y=k | \bm{x}) & = \frac{p(y=k) p(\bm{x} | y=k)}{p(\bm{x})} \\
                    & \propto p(y=k) p(x_1, \ldots, x_{\nfeat} | y=k) \\
                    & \approx p(y=k) \prod_{i=1}^{\nfeat}{p(X_i = x_i | y=k)}.
                \end{aligned}
            \end{equation}
        When the features are discrete, we estimate $p(y=k)$ and $p(X_i = x_i | y=k)$ for all $i$ and $k$ from training data.
        When the features are continuous ($X_i$ here means the $i$-th feature), the conditional probability $p(X_i = x_i | y=k)$  is usually modeled by a one-dimensional Gaussian distribution, i.e., $\mathcal{N}(x_i; \mu_{ki}, \sigma_{ki})$, where $\mu_{ki}$ and $\sigma_{ki}$ are estimated from data.
        
    \subsubsection{SVM}
        For convenience we discuss linear SVM in the case of separable data $\mathcal{S}$.
        Let $\mathcal{H}$ be the hypothesis set of linear classifiers, i.e., $\mathcal{H}=\Set{\Sign{\bm{w}^\top \bm{x} + b}}{\bm{w}, b}$.
        In $\R^\nfeat$, a hyperplane is represented as follows:
            \begin{equation}
                \bm{w}^\top \bm{x} + b = 0.
            \end{equation}
        Since the data is separable, there must be a hyperplane that does not pass any data; this means we do not have any constraints on $\bm{w}$ and $\bm{b}$.
        As a result, to avoid scaling issues (i.e., two hypotheses $(\bm{w}_1, b_1)$ and $(\bm{w}_2, b_2)$ correspond to the same hyperplane, but they have different magnitudes), we focus on a specific hyperplane $(\bm{w}, b)$ that satisfies the following:
            \begin{equation}
                \min_{(\bm{x}, y) \in \mathcal{S}} \Abs{\bm{w}^\top \bm{x} + b} = 1.
            \end{equation}
        
        The margin $\rho$ of an SVM is defined as the smallest distance of any data point to the separating hyperplane, i.e., 
            \begin{equation}
                \rho = \min_{(\bm{x}, y) \in \mathcal{S}} \underbrace{\frac{\Abs{\bm{w}^\top \bm{x} + b}}{\norm{\bm{w}}}}_{\text{distance of a point to the hyperplane}} = \frac{1}{\norm{\bm{w}}}.
            \end{equation}
        One motivation of SVM is to find the separating hyperplane with the largest margin; intuitively, such a hyperplane provides the largest cushion/protection.
        Thus, the optimization problem of the SVM is formulated below:
            \begin{equation}
                \begin{aligned}
                    & \min_{\bm{w}, b} & & \frac{1}{2}\norm{\bm{w}}^2 \\
                    & s.t.             & & y_i \cdot (\bm{w}^\top \bm{x}_i + b) \ge 1, \forall i,
                \end{aligned}
            \end{equation}
        where the constraints ensure that every sample is correctly classified.
        The Lagrange function of the above optimization is as follows where $\alpha_i \ge 0$:
            \begin{equation}
                L(\bm{w}, b, \bm{\alpha}) = \frac{1}{2}\norm{\bm{w}}^2 - \sum_{i=1}^{\ndata}{\alpha_i \left[ y_i \cdot (\bm{w}^\top \bm{x}_i + b) - 1\right]}.
            \end{equation}
        The KKT conditions are:
            \begin{equation}
                \begin{aligned}
                    \nabla_{\bm{w}} L & = \bm{w} - \sum_{i=1}^{\ndata}{\alpha_i y_i \bm{x}_i} = 0 \\
                    \nabla_{b} L & = -\sum_{i=1}^{\ndata}{\alpha_i y_i} = 0 \\
                    a_i & = 0 \text{ or } y_i(\bm{w}^\top \bm{x} + b) = 1, \forall i.
                \end{aligned}
            \end{equation}
        Several things from the KKT conditions:
            \begin{itemize}
                \item From the first condition: the weight vector $\bm{w}$ is a linear combination of $\bm{x}_i$; however, only those $\bm{x}_i$ with $\alpha_i \ne 0$ contribute to the combination.
                \item From the third condition: for those data such that $\alpha_i \ne 0$, we know $\Abs{\bm{w}^\top \bm{x} + b} = 1$, i.e., they are on the marginal hyperplanes.
                For separable data, there is only one type of support vectors, i.e., those on the marginal hyperplanes; however, if the data is non-separable, two types of support vectors exist: 1) those on the marginal hyperplanes and 2) outliers. 
            \end{itemize}
        The dual of the original optimization problem is as follows:
            \begin{equation}
                \begin{aligned}
                    & \max_{\bm{\alpha}} & & \sum_{i=1}^{\ndata}{\alpha_i} - \frac{1}{2}\sum_{i,j=1}^{\ndata}{\alpha_i \alpha_j y_i y_j (\bm{x}_i^\top \bm{x}_j) } \\
                    & s.t. & & \alpha_i \ge 0, \sum_{i=1}^{\ndata}{\alpha_i y_i} = 0.
                \end{aligned}
            \end{equation}
        Several useful things from the dual:
        \begin{itemize}
            \item Only the support vectors affect the solutions of the SVM.
            \item The support vectors are on the marginal hyperplanes, which are \emph{parallel} to the separating hyperplane $(\bm{w}, b)$.
            Thus, for any support vector $\bm{x}_i$ we have $\bm{w}^\top \bm{x}_i + b = y_i$.
            It follows that $b = y_i - \sum_{j=1}^{\ndata}{\alpha_i y_i (\bm{x}_j^\top \bm{x}_i) }$.
            \item Kernel: we can map $\bm{x}_i$ to some high dimensional space (i.e., $\phi(\bm{x}_i)$), as long as we can efficiently compute the inner product $\phi(\bm{x}_i)^\top \phi(\bm{x}_j)$.
        \end{itemize}
    
    
\subsection{Boosting}
\subsubsection{AdaBoost}
The intuition behind AdaBoost is to generate a strong classifier (one with small generalization error) by combining a bunch of weak classifiers. 
A typical algorithm of boosting is called AdaBoost; it typically consists of $T$ iterations of boosting, where a weak classifier is picked in each iteration; finally, the $T$ weak classifiers are combined in a way that a strong classifier is generated. 
The pseudocode of AdaBoost is displayed in Algorithm~\ref{adaboost:algo}~\cite{mohri2018foundations}.

\begin{algorithm}[ht]
\caption{Pseudocode for AdaBoost}\label{adaboost:algo}
\begin{algorithmic}[1]
\State Input: \ndata labeled samples $\mathcal{S}=\SET{(\bm{x}_1, y_1), \ldots, (\bm{x}_\ndata, y_\ndata)}$, where $y_i \in \SET{-1, 1}$
\For{$i=1,\ldots, \ndata$}
    \State $D_1(i) = 1 / \ndata$ \Comment{the weights on the \ndata samples are initially uniform}
\EndFor
\For{$t=1,\ldots, T$} \Comment{doing $T$ iterations of boosting}
    \State $h_t \leftarrow \argmin_{h \in \mathcal{H}} \sum_{i=1}^{\ndata}{D_t(i) \mathbbm{1}[h(\bm{x}_i \ne y_i)]}$; the error of $h_t$ is $\epsilon_t$. \label{adaboost:pick-classifier}
    \State $\alpha_t = \frac{1}{2}\log \frac{1-\epsilon_t}{\epsilon_t}$ \label{adaboost:weight-classifier}
    \State $Z_t = 2 \left[ \epsilon_t(1 - \epsilon_t) \right]^{1/2}$ \Comment{the normalization constant to make sure that $D_t(i)$ sums to one}
    \For{$i=1,\ldots, \ndata$} \label{adaboost:weight-sample}
        \State $D_{t+1}(i) = \frac{D_t(i)\EXP{-\alpha_t y_i h_t(\bm{x}_i)}}{Z_t}$ \Comment{update the weight associated with each sample}
    \EndFor
\EndFor
\State $g = \sum_{t=1}^{T}{\alpha_t h_t}$ \Comment{generate a strong classifier by combining weak learners}
\State return $h=\Sign{g}$
\end{algorithmic}
\end{algorithm}

There are several key questions:
    \begin{enumerate}
        \item How to pick a weak class in a particular iteration $t$? The answer is intuitive: picking the classifier $h \in \mathcal{H}$ that attains the smallest empirical error on the data, as showed in step \ref{adaboost:pick-classifier}.
        It is worth mentioning that the empirical error is weighted by $D(i), \ldots, D(\ndata)$.
        \item How to combine these picked weak classifiers? In AdaBoost, the strong classifier is just a linear combination of $h_1, \ldots, h_T$.
        \item How to set the weight $\alpha_t$ associated with each weak classifier? The values of $\alpha_1, \ldots, \alpha_T$ are selected such that an upper-bound on the empirical error of $g$ is minimized. See the proof of Theorem 6.1 at~\cite{mohri2018foundations}.
    \end{enumerate}

    \subsubsection{Boosting under the FSAM Framework}\label{sec:boosting-trees}
        Boosting is a framework where the weak learners are either classification or regression trees.
        Boosting is based on the \emph{Forward Stagewise Additive Modeling}~(FSAM) described in~\cite{hastie2009elements} (p.342), which is a great framework to generate a strong predictor (either classifiers or regressors) from a set of weak predictors.
        The framework is to find a linear combination of a set of $K$ learners such that some loss is minimized, i.e., 
            \begin{equation}
                \min_{\SET{\theta_i}_{i=1}^{K}, \SET{\beta_i}_{i=1}^{K} } \sum_{i=1}^{\ndata}{L\left( y_i, \sum_{j=1}^{K}{\beta_j f(\bm{x}_i; \theta_j)} \right)},
            \end{equation}
        where $\SET{\theta_i}_{i=1}^{K}$ are model parameters for the learners and $\SET{\beta_i}_{i=1}^{K}$ are the weights to combine them together.
        It is usually hard to solve the optimization above; instead, an iterative greedy approach is used, where each iteration learns both a single learner and the weight associated with it, i.e., 
            \begin{equation}\label{eq:forward-stagewise-add-model}
                \min_{\theta_m, \beta_m} \sum_{i=1}^{\ndata}{
                    L\left( y_i, F_{m-1}(\bm{x}_i) + \beta_m f(\bm{x}_i; \theta_m) \right)
                },
            \end{equation}
        where $F_{m-1}(\bm{x}_i) = \sum_{j=1}^{m-1}{\beta_m f(\bm{x}_i; \theta_j)}$ combines the learners that have been learned so far. 
        As boosting sequentially reduces the training loss, the bias increases as we combine more learners; intuitively, the combined model becomes more complicated.
        
    \subsubsection{AdaBoost}
         An instantiation of \eqref{eq:forward-stagewise-add-model} with the loss function $L(\cdot)$ replaced by the exponential loss leads to the AdaBoost algorithm.
        In the $m$-th iteration:
            \begin{equation}
                \begin{aligned}
                    L & = \sum_{i=1}^{\ndata}{ \EXP{ - y_i \cdot \left[F_{m-1}(\bm{x}_i) + \beta_m f(\bm{x}_i; \theta_m) \right] } }\\
                      & = \sum_{i=1}^{\ndata}{\underbrace{\EXP{-y_i \cdot F_{m-1}(\bm{x}_i)}}_{\text{the weight of $\bm{x}_i$ at iteration $m$}} \cdot \EXP{-y_i \cdot \beta_m f(\bm{x}_i; \theta_m)} }\\
                      & = \sum_{i=1}^{\ndata}{w_i^{(m)} \cdot \EXP{-y_i \cdot \beta_m f(\bm{x}_i; \theta_m)}} \\
                      & = \sum_{i:y_i = f(\bm{x}_i;\theta_m)}^{}{w_i^{(m)} \EXP{-\beta_m}} + \sum_{i:y_i \ne f(\bm{x}_i;\theta_m)}^{}{w_i^{(m)} \EXP{\beta_m}} \\
                      & = \sum_{i=1}^{\ndata}{w^{(m)}_i e^{-\beta_m} \left( 1- \mathbbm{1}[y_i \ne f(\bm{x}_i; \theta_m)] \right)} + \sum_{i=1}^{\ndata}{w^{(m)}_i e^{\beta_m} \mathbbm{1}[y_i \ne f(\bm{x}_i; \theta_m)]} \\
                      & = \sum_{i=1}^{\ndata}{w^{(m)}_i e^{-\beta_m}} - \sum_{i=1}^{\ndata}{w^{(m)}_i e^{-\beta_m} \mathbbm{1}[y_i \ne f(\bm{x}_i; \theta_m)]} + \sum_{i=1}^{\ndata}{w^{(m)}_i e^{\beta_m} \mathbbm{1}[y_i \ne f(\bm{x}_i; \theta_m)]} \\
                      & = (e^{\beta_m} - e^{-\beta_m}) \underbrace{\sum_{i=1}^{\ndata}{w^{(m)}_i \mathbbm{1}[y_i \ne f(\bm{x}_i; \theta_m)]}}_{\epsilon_m: \text{weighted empirical error}} + e^{-\beta_m}.
                \end{aligned}
            \end{equation}
        Fixing $\beta_m$, the AdaBoost picks the learner that minimizes the weighted error $\epsilon_m$, i.e., $\theta_m = \argmin_{\theta} \epsilon_m$.
        Notice that $\epsilon_m$ does not depend on $\beta_m$.
        After obtaining the best learner at iteration $m$, the corresponding error $\epsilon_m$ is fixed.
        Then, the value of $\beta_m$ is obtained by solving the following convex programming:
        \begin{equation}
            \beta_m = \argmin_{\beta} (e^{\beta_m} - e^{-\beta_m}) \cdot \epsilon_m + e^{-\beta_m}.
        \end{equation}
         Some practical considerations:
        \begin{itemize}
            \item In practice $f(\bm{x}_i; \theta_m)$ is usually implemented as a stump, i.e., a tree with a single level.
            Thus, minimizing $\epsilon_m$ consists of two steps: 1) pick a particular feature and 2) pick a threshold for the feature, such that the resulting weighted empirical error $\epsilon_m$ is minimized.
        \end{itemize}
        
        
        
        
\subsubsection{Gradient Boosting (GB)}
        GB is still under the FSAM framework.
        In the $m$-th iteration, GB is to solve the following optimization:
            \begin{equation}\label{XGBoost-model}
                \min_{f^{(m)}} \sum_{i=1}^{\ndata}{
                    \underbrace{\left( y_i - (\hat{y}^{(m-1)}_i + f^{(m)}(\bm{x}_i)) \right)^2}_{:=L(y_i, \hat{y}_i^{(m)}) }.
                }
            \end{equation}
        We do a first-order Taylor expansion of $L(y_i, \hat{y}_i^{(m)})$ at $\hat{y}_i^{(m-1)}$:
            \begin{equation}
                L(y_i, \hat{y}_i^{(m)}) \approx L(y_i, \hat{y}_i^{(m-1)}) + \underbrace{\left[ \frac{\partial L(y_i, z)}{\partial z}\vline_{z = \hat{y}_i^{(m-1)}} \right]}_{g_{i}^{(m)}}  f^{(m)}(\bm{x}_i).
            \end{equation}
        A natural idea is to let $f^{(m)}(\bm{x}_i)$ be as close as possible to $-g^{(m)}_i$, such that the reduction of $L(y_i, \hat{y}_i^{(m)})$ is along the direction of the steepest descent.
        Thus, the learning problem  in the $m$-th iteration is as follows:
            \begin{itemize}
                \item  Collect a new dataset $\mathcal{G}=\SET{(\bm{x}_1, -g^{(m)}_1), \ldots, (\bm{x}_\ndata, -g^{(m)}_\ndata)}$, where the supervised information are the gradients.
                \item  Train a regression/classification tree on $\mathcal{G}$, which gives us $f^{(m)}(\cdot)$. 
                \item  Find the weight associated with $f^{(m)}(\cdot)$ by solving $\min_{\gamma} \sum_{i=1}^{\ndata}{L\left(y_i, \hat{y}_i^{(m-1)} + \gamma f^{(m)}(\bm{x}_i)\right)}$.
            \end{itemize}


    \subsubsection{XGBoost}
        This is another instantiation of the FSAM framework.
        The optimization problem that XGBoost solves at iteration $m$ is as follows:
            \begin{equation}\label{XGBoost-model}
                \min_{f^{(m)}} \sum_{i=1}^{\ndata}{
                    \underbrace{\left( y_i - (\hat{y}^{(m-1)}_i + f^{(m)}(\bm{x}_i)) \right)^2}_{:=L(y_i, \hat{y}_i^{(m)}) } + R(f^{(m)}),
                }
            \end{equation}
        where $f^{(m)}$ is the learner that needs to be learned at the current iteration.
        Notice that XGBoost has an additional regularization term $R(f^{(m)})$ to control the learner's complexity.
        We approximate the loss $L(y_i, \hat{y}_i^{(m)})$ with its second-order Taylor expansion at $\hat{y}^{(m-1)}_i$:
            \begin{equation}
                L(y_i, \hat{y}_i^{(m)}) \approx L(y_i, \hat{y}^{(m-1)}_i) + g_i \cdot f^{(m)}(\bm{x}_i) + \frac{1}{2} h_i \cdot (f^{(m)}(\bm{x}_i))^2,
            \end{equation}
        where $f^{(m)}(\bm{x}_i) = (\hat{y}_i^{(m)} - \hat{y}_i^{(m-1)})$, $g_i = \frac{\partial L(y_i, \hat{y}_i^{(m-1)})}{\partial \hat{y}_i^{(m-1)} }$, and $h_i = \frac{\partial^2 L(y_i, \hat{y}_i^{(m-1)})}{\partial^2 \hat{y}_i^{(m-1)} }$.
        We regularize the complexity of $f^{(m)}$ by the following:
            \begin{equation}
                R(f^{(m)}) = \gamma T + \frac{\lambda}{2}\sum_{i=1}^{T}{s_i},
            \end{equation}
        where $T$ is the number of leaves and $s_i$ the final score at each leave; for regression problems $s_i$ is the mean of the target values of the points that belong to the region associated with the leave; for classification problems $s_i$ is the probability of the most likely class.
        Then, the optimization problem of XGBoost (i.e., \eqref{XGBoost-model}) can be approximated by solving the following:
            \begin{equation}
                \begin{aligned}
                    \tilde{L} & = \sum_{i=1}^{\ndata}{
                        \left[
                            g_i \cdot f^{(m)}(\bm{x}_i) + \frac{1}{2} h_i \cdot (f^{(m)}(\bm{x}_i))^2 
                        \right] + \gamma T + \frac{\lambda}{2}\sum_{i=1}^{T}{s_i}
                    } \\
                    & \quad \quad \text{$s_j = f^{(m)}(\bm{x}_i)$ for any $i \in \mathcal{I}_j$, i.e., all instances belonging to leave $j$ get the same score.} \\
                    & = \sum_{j=1}^{T}{
                        \left[ \underbrace{\left(\sum_{i \in \mathcal{I}_j}^{}{g_i} \right)}_{:=G_j} s_j + \frac{1}{2}\left( \underbrace{\sum_{i \in \mathcal{I}_j}^{}{h_i}}_{:=H_j} + \lambda \right)s_j^2
                        \right] + \gamma T 
                    } \\
                    & = \sum_{j=1}^{T}{ \left[ G_j s_j + \frac{1}{2} H_j s_j^2 \right] } + \gamma T.
                \end{aligned}
            \end{equation}
        When $s_j = -\frac{G_j}{H_j + \lambda}$, the loss $\tilde{L}$ attains its minimum value, which is also called the structure score of $f^{(m)}$:
            \begin{equation}
                \text{structure score of $f^{(m)}$}= -\frac{1}{2}\sum_{j=1}^{T}{\frac{G_j^2}{H_j + \lambda}} + \gamma T.
            \end{equation}
        Ideally, in each iteration we would like to enumerate all possible tree structures and pick the one that has the smallest structure score; however, this is too expensive.
        Instead, XGBoost grows a \emph{single} tree by iteratively splitting a node into the left and right nodes (just as how we splitted in decision trees).
        We choose the feature $i$ that maximizes the following gain as the splitting feature:
            \begin{equation}
                \frac{1}{2}\left[ \frac{G_L^2}{H_L + \lambda} + \frac{G_R^2}{H_R + \lambda} - \frac{(G_L + G_R)^2}{H_L +  H_R + \lambda}  \right] - \gamma
            \end{equation}
        Check out this excellent article about GB: \url{https://xgboost.readthedocs.io/en/stable/tutorials/model.html}.
        The Wikipedia has a good introduction for generic GB: \url{https://en.wikipedia.org/wiki/Gradient_boosting}.
    
        
        \paragraph{Generic GB v.s. XGBoost.}
        In each iteration of a generic GB, the first-order gradient of the loss w.r.t. the predictor of the last iteration is used as supervised information to train a weak learner, i.e., $f^{(m)}(\bm{x})$ should be close to $-\frac{\partial L}{\partial f^{(m-1)}(\bm{x})}$.
        However, XGBoost uses both the first- and second-order gradient information as a criterion to split features.
        
    \subsubsection{Missing Values}
        A major advantage of gradient boosting trees (GBT) is the capability of dealing with missing values.
        Consider the following two settings:
            \begin{itemize}
                \item Both the training and testing data have missing values at some features: GBT determines the optimal split of these features based on training data, which is used when handling missing values during testing.
                \item Only the testing data has missing values: the testing data with missing values is split into the majority class (computed from the training data).
            \end{itemize}
        
    
    
    
\subsection{Bagging}

Another technique with the idea of combining weak learners to generate a strong learner is \emph{bagging}.
Intuitively, given a dataset $\mathcal{S}$ with \ndata samples, bagging consists of three steps:
    \begin{enumerate}
        \item Bootstrap: sample $k$ subsets of $\mathcal{S}$ with replacement. 
        \item Parallel training: training $k$ weak learners on the subsets in paralle. 
        \item Aggregation: given a test sample, aggregate the $k$ weak learners by computing (in the case of regression tasks) their average or (in the case of classification tats) the majority voting.
    \end{enumerate}
A key advantage of bagging is to reduce variance\footnote{https://www.ibm.com/cloud/learn/bagging}.
Intuitively, let $L_i$ be the generalization error of the model trained on the $i$-th bootstrapped sample.
Assume that $L_1, \ldots, L_k$ are not correlated; this is not true in theory, however, it helps the subsequent analysis.
The variance after bagging is $\text{Var}\left(\frac{\sum_{i=1}^{k}{L_i}}{k}\right) = \frac{\text{Var}(L_i)}{k}$.
\footnote{For uncorrelated random variables $L_1, \ldots, L_k$: $\text{Var}\left(\frac{\sum_{i=1}^{k}{L_i}}{k}\right) = \frac{1}{k^2} \sum_{i=1}^{k}{\text{Var}(L_i)} = \frac{k \text{Var}(L_i)}{k^2} =  \frac{ \text{Var}(L_i)}{k}$.}
Thus, the variance of the bagged model is reduced by a factor of $k$.



\subsection{Clustering}
    \subsubsection{K-Means Clustering}
        \begin{itemize}
             
            \item Given an unlabeled data $\mathcal{S}=\SET{\bm{x}_1, \ldots, \bm{x}_\ndata}$, the loss function that K-Means clustering minimizes is:
            \begin{equation}
                \min_{\bm{c}_1, \ldots, \bm{c}_R} \sum_{i=1}^{R}{
                    \sum_{j \in \mathcal{C}_i}^{}{\norm{\bm{x}_j - \bm{c}_i}}
                },
            \end{equation}
            where $\bm{c}_1, \ldots, \bm{c}_R$ are the cluster centers that are  randomly initialized.
            
            \item The K-Means alternates the two steps until convergence: 1) assign each sample to a cluster and 2) update the centers.
            Formally, the steps are as follows:
            \begin{enumerate}
                \item Assign $\bm{x}_i$ to cluster $k \in \argmin_{j=1,\ldots, R}\norm{\bm{x}_i - \bm{c}_j}_2^2$.
                \item Update the center of each cluster, i.e., $\bm{c}_k = \frac{\sum_{j \in \mathcal{C}_k}^{}{\bm{x}_j}}{\SetCard{\mathcal{C}_k}}$.
            \end{enumerate}
            \item Notice that K-Means clustering can also be used as a classification algorithm. 
            \item The above two steps can be thought of as an instantiation of the famous EM algorithm; as a result, the resulting solutions (i.e., the clustering centers) are only sub-optimal w.r.t. the loss function.
            \item How to choose $K$? We can find $K$ by solving $\min_{K} \frac{B(K)}{W(K)}$, where $B(K)$ is the normalized total between-cluster distance and $W(K)$ the normalized total within-cluster distance. 
            A commonly used method to choose $K$ is the elbow method.
        \end{itemize}
    
    \subsubsection{Spectral Clustering}
        Given an unlabeled data $\mathcal{S}=\SET{\bm{x}_1, \ldots, \bm{x}_\ndata}$, we can think of the \ndata instances as \ndata vertices of a graph.
        An edge between a pair of vertices $(i, j)$ represents the similarity measure between $\bm{x}_i$ and $\bm{x}_j$.
        The adjacency matrix $\bm{A}$ of the graph encodes the similarity between points, i.e., $A_{i,j}$ is the similarity between $i$ and $j$.
        The objective of the spectral clustering is to find $K$ clusters $\mathcal{C}_1, \ldots, \mathcal{C}_K$ of the vertices, such that the within cluster (resp. between cluster) similarity is maximized (resp. minimized).
        One way to capture the objective is by minimizing the following loss function:
            \begin{equation}
                L = \sum_{i=1}^{K}{\frac{1}{\SetCard{\mathcal{C}_i}} \sum_{u \in \mathcal{C}_i, v \notin \mathcal{C}_i}^{}{A_{u, v}}}.
            \end{equation}
        Let $\bm{H} \in \R^{\ndata \times K}$ be the matrix encoding which cluster a point belongs to, i.e., $H_{i,j}=\frac{1}{\sqrt{\SetCard{\mathcal{C}_j}}}$ if $\bm{x}_i$ belongs to cluster $\mathcal{C}_j$; otherwise $H_{i,j}=0$.
        Note that the columns of $\bm{H}$ are orthogonal with each other. 
        Let $\bm{h}_i$ be the $i$-th column of $\bm{H}$.
        The loss function $L$ can also be formulated with the Laplacian of the underlying graph:
            \begin{equation}
                \begin{aligned}
                    L  = \Tr\left( \bm{H}^\top \bm{L} \bm{H}  \right) = \sum_{i=1}^{K}{\bm{h}_i^\top \bm{L} \bm{h}_i} 
                      = \sum_{i=1}^{K}{\bm{h}_i^\top \left( \bm{D} - \bm{A} \right) \bm{h}_i} & = \sum_{i=1}^{K}{\left(   \sum_{k \in \mathcal{C}_i}^{}{d_k \frac{1}{\SetCard{\mathcal{C}_i}}} - \sum_{u, v \in \mathcal{C}_i}^{}{\frac{1}{\SetCard{\mathcal{C}_i}}A_{u, v}} 
                      \right)} \\
                      & =   \sum_{i=1}^{K}{\left( \frac{1}{\SetCard{\mathcal{C}_i}} \left[ \sum_{k \in \mathcal{C}_i}^{}{d_k} - \sum_{u, v \in \mathcal{C}_i}^{}{A_{u, v}} \right] \right)} \\
                      & = \sum_{i=1}^{K}{\frac{1}{\SetCard{\mathcal{C}_i}} \sum_{u \in \mathcal{C}_i, v\notin \mathcal{C}_i}^{}{A_{u,v}}}.
                \end{aligned}
            \end{equation}
        Minimizing $L$ directly is usually intractable since it is an integer programming.t
        Instead, we use the following procedure to obtain an approximate solution~\cite{shalev2014understanding}:
            \begin{enumerate}
                \item Get the SVD of the Laplacian $\bm{L}$ and set $\bm{U} \in \R^{\ndata \times K}$ as the matrix whose columns are the $K$ eigenvectors (ignoring the trivial constant eigenvector) corresponding to the $K$ \emph{smallest} eigenvalues of $\bm{L}$.
                \footnote{It is worth noting that the Laplacian matrix of a connected, undirected graph is a positive semidefinite matrix; this is because it is a real symmetric diagonally dominant matrix.}
                \item The $K$ clusters $\mathcal{C}_1,\ldots, \mathcal{C}_K$ are obtained by running a K-Means clustering on the rows of $\bm{U}$.
            \end{enumerate}
        
        
\subsection{K-Nearest Neighbor (KNN)}
One thing worth noting about the KNN model is that the parameter $K$ is inversely related to the model complexity, i.e., 
    \begin{itemize}
        \item When $K$ is small, e.g., $K=1$, the model complexity is high. So the bias is small; however, the variance will be large, that is, the prediction of a testing instance varies a lot when the training data changes. 
        \item When $K$ is large, the bias is large and the variance is small.
        \item The implementation of KNN utilizes a data structure called K-d tree. 
        \item In practice, approximate KNN is usually good enough, which can be done by either the locality sensitive hashing (LSH) technique, or the  Hierarchical Navigable Small World graphs (HNSW) approach.
        \footnote{A discussion of KNN from the perspective of industry is here: \url{https://www.elastic.co/blog/introducing-approximate-nearest-neighbor-search-in-elasticsearch-8-0}.}
    \end{itemize}


\subsection{Decision Trees}
    \subsubsection{Theoretical Background}
    \begin{itemize}
        \item A decision tree with $k$ leaves can shatter a set of $k$ instances; thus, if we allow for a tree with arbitrary size, overfitting is an issue.
        \item We often use \emph{minimum description length} (MDL) to measure the complexity of a decision tree. Notice that before applying MDL, we need to have a countable hypothesis set $\mathcal{H}$ and some description language (e.g., binary strings) to describe the hypothesis in $\mathcal{H}$.
        \item The generalization error $L_\mathcal{D}(h)$ of the decision tree $h$ satisfies the following:
            \begin{equation}
                L_\mathcal{D}(h) \le \underbrace{L_\mathcal{S}(h)}_{\text{empirical error}} + \sqrt{\frac{\SetCard{h} + \log(2/\delta)}{2m}},
            \end{equation}
        where $\SetCard{h}$ is the complexity of $h$.
        Based on~\cite{shalev2014understanding}(page 251), a binary decision tree with $n$ nodes can be described by $(n+1)\log_2(\nfeat+2)$ bits, where $\nfeat$ is the feature dimension.
        Thus, the second term on the right-hand side becomes:
            \begin{equation}
                \sqrt{\frac{\SetCard{h} + \log(2/\delta)}{2m}} = \sqrt{\frac{(n+1)\log_2(\nfeat+2) + \log(2/\delta)}{2m}}.
            \end{equation}
        \item A general learning framework to learn a decision tree is as follows:
            \begin{equation}
                h^\ast \in \argmin_{h \in \mathcal{H}} L_{\mathcal{D}}(h).
            \end{equation}
    \end{itemize}

    \subsubsection{Algorithms}
    \paragraph{ID3 Algorithm.} Some high-level description of the classic ID3 algorithm on a dataset with binary features is as follows.
    The $\text{ID3}(\mathcal{X}, \mathcal{F})$ has two inputs: 1) the data $\mathcal{X}$ and 2) the features $\mathcal{F}$.
    Intuitively, we pick a feature $j \in \mathcal{F}$ based on some gain measures; then, we split $\mathcal{X}$ into two subsets depending on whether $j$-th feature is one or zero; finally, we recursively run ID3 on each subset  with the new feature set $\mathcal{F} \setminus \SET{j}$, i.e., $\text{ID3}(\mathcal{X}_1, \mathcal{F} \setminus \SET{j})$ and $\text{ID3}(\mathcal{X}_0, \mathcal{F} \setminus \SET{j})$, where $\mathcal{X}_1 = \Set{\bm{x}}{x_j = 1}$ (similar definition applies for $\mathcal{X}_0$).
    The C4.5 algorithm is similar to the ID3. 
    Check out \cite{shalev2014understanding} page 253 for details. 

    \paragraph{Regression Tree (or CART).} 
    When features are of continuous values, a regression tree is different from a decision tree in two major ways:
        \begin{itemize}
            \item The way we split a node: suppose we split feature $j$ with threshold $\theta_j$.
            Let $\mathcal{X}_l$ and $\mathcal{X}_r$ be the splitted subsets. 
            The target value associated with a sample $i$ is represented by $y_i$.
            The gain of this split is called \emph{variance reduction}:
            $\text{Var}(y_i | i \in \mathcal{X}) - \left( \text{Var}(y_i | i \in \mathcal{X}_l) + \text{Var}(y_i | i \in \mathcal{X}_r) \right)$; for example, $\text{Var}(y_i | i \in \mathcal{X}) = \frac{1}{\SetCard{\mathcal{X}}}\sum_{i \in \mathcal{X}}^{}{(y_i - \bar{y}_i)^2}$. Intuitively, we want to choose the feature $j$ as the splitting feature such that the resulting variance reduction is maximized.
            \item The way we predict the target value of a testing sample: given a testing sample, the prediction is done by traversing the sample from the root down to a leaf.
            The leaf corresponds to a subset of samples; thus, the prediction is the average of the samples' target values.
            Check out the detailed implementations of decision trees: \url{https://github.com/ddbourgin/numpy-ml/blob/master/numpy_ml/trees/dt.py}.
        \end{itemize}
        
    \paragraph{Common Impurity Measures.}
    For a particular node $k$, let $f^k_i$ be the frequency of label $i$ and $C$ the total number of unique labels at the node.
    \begin{itemize}
        \item Gini impurity (classification): $\sum_{i=1}^{C}{f^k_i (1 - f^k_i)}$.
        \item Entropy (classification): $-\sum_{i=1}^{C}{f^k_i \log f^k_i}$.
        \item Variance (regression): $\frac{1}{N}\sum_{i=1}^{N}{(y^k_i - \mu)^2}$, where $N$ is the total number of instances at node $k$, $y^k_i$ is the label for a particular instance, and $\mu$ is the mean of the labels at the node, i.e., $\frac{1}{N}\sum_{i=1}^{N}{y^k_i}$.
    \end{itemize}
    
    
    
    \paragraph{Random Forests (RF).} It is worth noting that RF is similar to Boosting Trees (BT) from a modeling perspective, i.e., they are both an ensemble of decision trees.
    The difference is in the training method:
        \begin{itemize}
            \item Training BT is more complicated than training RF, the former uses a framework called \emph{Forward Stagewise Additive Modeling}; see \ref{sec:boosting-trees} for details.
            % In addition, BT usually trains a single tree.
            \item RF is trained by bootstrapping subsets from the original data and training a decision tree on every subset. 
            The final output is the average of the decision trees.
        \end{itemize}



\subsection{Expectation-Maximization Algorithm (EM)}
    The EM algorithm is most suitable for maximum likelihood estimation problems of models with latent variables.
    Consider a dataset (not necessarily labeled) $\mathcal{D} = \SET{\bm{x}_1, \ldots, \bm{x}_\ndata}$.
    Suppose we want to fit a two-component Gaussian mixture model on $\mathcal{D}$.
    The log-likelihood function is:
        \begin{equation}
            \log L(\mathcal{D}) = \sum_{i=1}^{\ndata}{\log P(\bm{x}_i)} = \sum_{i=1}^{\ndata}{\log\left[ (1-\pi)f_1(\bm{x}_i; \theta_1) + \pi f_2(\bm{x}_i; \theta_2) \right]},
        \end{equation}
    where $\pi$ is the probability that an instance was sampled from the second component; $f_i(\bm{x}_j; \theta_i)$ is just a Gaussian probability density function with $\theta_i$ including the mean and covariance.
    The model is latent in the sense that each instance is associated with a latent variable indicating which component it was sampled from, i.e., $m_i$ indicates the component membership of $\bm{x}_i$.
    If we knew $\mathcal{M} = \SET{m_1, \ldots, m_\ndata}$, the resulting log-likelihood function should be:
        \begin{equation}
            \begin{aligned}
            \log L(\mathcal{D}, \mathcal{M}) & = \sum_{i=1}^{\ndata}{
                \left[
                    (1 - m_i) \log f_1(\bm{x}_i; \theta_1) + m_i  \log f_2(\bm{x}_i; \theta_2) + 
                    (1 - m_i) \log(1 - \pi) + m_i \log\pi
                \right]
            }.
            \end{aligned}
        \end{equation}
    The EM algorithm is to maximize $\log L(\mathcal{D}, \mathcal{M})$ by alternating the following two steps:
        \begin{itemize}
            \item Expectation (E-step): Suppose $\theta_1$ and $\theta_2$ are known. 
            Let the estimates of the memberships from the previous iteration be $m'_1, \ldots, m'_\ndata$.
            As $\mathcal{M}$ is actually not available, we use $P(m_i=1 | \theta_1, \theta_2, m'_i)$ in place of $m_i$, which is computed as follows:
                \begin{equation}
                    \begin{aligned}
                    P(m_i=1 | \theta_1, \theta_2, m'_i) & = \frac{P(i \in \mathcal{C}_2) P(\bm{x}_i | i \in \mathcal{C}_2)}{P(\bm{x}_i)} \\
                    & = \frac{\left( \sum_{j=1}^{\ndata}{m'_j} / \ndata \right) f_2(\bm{x}_i; \theta_2)}{ \left( \sum_{j=1}^{\ndata}{m'_j} / \ndata \right) f_2(\bm{x}_i; \theta_2) + \left( \sum_{j=1}^{\ndata}{(1-m'_j)} / \ndata \right) f_1(\bm{x}_i; \theta_1) }.
                \end{aligned}
                \end{equation}
            \item Maximization (M-step): In this step we estimate $\theta_1$ and $\theta_2$ based on $P(m_i=1)$ computed in the E-step.
            We use the estimation of $\theta_2$ as an example:
                \begin{equation}
                    \begin{aligned}
                        \hat{\bm{\mu}}_2 & = \frac{\sum_{i=1}^{\ndata}{P(m_i=1) \bm{x}_i}}{\sum_{i=1}^{\ndata}{P(m_i=1)}}  \\
                        \hat{\bm{\Sigma}}_2 & = \frac{\sum_{i=1}^{\ndata}{P(m_i=1)\left( \bm{x}_i -  \hat{\bm{\mu}}_2  \right)\left( \bm{x}_i -  \hat{\bm{\mu}}_2  \right)^\top}}{\sum_{i=1}^{\ndata}{P(m_i=1)}}.
                    \end{aligned}
                \end{equation}
        \end{itemize}
        
        \subsubsection{A Concrete Example of EM Algorithm}
        This example is based on the network structure estimation paper from~\citet{newman2018estimating}.
        Suppose we would like to estimate the adjacency matrix $\bm{A}$ of a network.
        We assume a probabilistic model $P(\bm{A} | \gamma)$ of the structure, with parameters $\gamma$.
        In addition, suppose we have a dataset $\mathcal{D}$ at hand, which can be any kind of measurement from the network.
        Similarly, we assume a probabilistic model underlying the data, i.e., $P(\mathcal{D} | \theta)$ parameterized by $\theta$.
        The goal is to estimate both $\gamma$ and $\theta$.
        From Bayes rule, the posterior distribution of both the parameters and the adjacency matrix is:
            \begin{equation}
                P(\bm{A}, \gamma, \theta | \mathcal{D}) =  \frac{P(\bm{A}|\gamma) P(\mathcal{D} | \theta) P(\theta) P(\gamma)}{P(\mathcal{D})},
            \end{equation}
        where $P(\theta)$ and $P(\gamma)$ are the prior distributions of the parameters.
        The posterior distribution of $\gamma$ and $\theta$ is obtained by marginalizing $\bm{A}$, i.e., 
            \begin{equation}
                P(\gamma, \theta | \mathcal{D}) = \sum_{\bm{A}}^{}{P(\bm{A}, \gamma, \theta | \mathcal{D})}.
            \end{equation}
            
        Notice that $P(\gamma, \theta | \mathcal{D})$ is the distribution that we care about, e.g., we can obtain  estimation of the parameters by maximizing the posterior (MAP).
        However, the marginalization is intractable, which makes directly maximizing $P(\gamma, \theta | \mathcal{D})$ over $\gamma$ and $\theta$ too hard.
        A more tractable way is to maximize a lower bound on the logarithm of $P(\gamma, \theta | \mathcal{D})$, i.e., 
            \begin{equation}
                \log P(\gamma, \theta | \mathcal{D}) = \log \sum_{\bm{A}}^{}{P(\bm{A}, \gamma, \theta | \mathcal{D})} \ge 
                \underbrace{\sum_{\bm{A}}^{}{q(\bm{A}) \frac{P(\bm{A}, \gamma, \theta | \mathcal{D})}{ q(\bm{A}) }}}_{(\ast)},
            \end{equation}
        where $q(\bm{A}) \ge 0$ and $\sum_{\bm{A}}^{}{q(\bm{A})} = 1$.
        The second inequality utilizes the Jensen's inequality.
        \footnote{For any set of positive quantities $x_i$: $\log \sum_{i}^{}{x_i} \ge \sum_{i}^{}{q_i \log \frac{x_i}{q_i}}$.}
        We can use an iterative algorithm to maximize $(\ast)$.
        The algorithm is to repeat the following steps until convergence:
            \begin{enumerate}
                \item Holding $\gamma$ and $\theta$ fixed, maximize $(\ast)$ w.r.t. $q(\bm{A})$ by setting
                    \begin{equation}
                        q(\bm{A}) = \frac{P(\bm{A}, \gamma, \theta | \mathcal{D})}{\sum_{\bm{A}}^{}{P(\bm{A}, \gamma, \theta | \mathcal{D})}}.
                    \end{equation}
                \item Holding $q(\bm{A})$ fixed, maximize $(\ast)$ w.r.t. $\gamma$ and $\theta$, i.e.,
                    \begin{equation}
                        \max_{\gamma, \theta}\left[ \sum_{\bm{A}}^{}{
                            q(\bm{A}) \log P(\bm{A}, \gamma, \theta | \mathcal{D})
                        } \right].
                    \end{equation}
                The maximization can be done by applying gradient-based methods to separately optimize over $\gamma$ and $\theta$.
                Specifically, we first expand the term inside the bracket as follows:
                    \begin{equation}
                        \sum_{\bm{A}}^{}{
                            q(\bm{A}) \log P(\bm{A}, \gamma, \theta | \mathcal{D})
                        } \propto \sum_{\bm{A}}^{}{
                            q(\bm{A}) \Big[  \log P(\mathcal{D} | \bm{A}, \theta) + \log P(\bm{A} | \gamma) + \log P(\gamma) + \log P(\theta) \Big]
                        }.
                    \end{equation}
                Thus, the gradients w.r.t. $\gamma$ and $\theta$ are:
                    \begin{equation}
                        \begin{aligned}
                        & \nabla_\gamma \log P(\gamma) + \sum_{\bm{A}}^{}{q(\bm{A}) \nabla_{\gamma} \log P(\bm{A} | \gamma)}
                         \\
                        & \nabla_\theta \log P(\theta) + \sum_{\bm{A}}^{}{q(\bm{A}) \nabla_\theta \log P(\mathcal{D} | \bm{A}, \theta) }.
                        \end{aligned}
                    \end{equation}
            \end{enumerate}
    
        

\subsection{Imbalanced Data}
    \paragraph{Downsampling and Upweighting.}
    Suppose we have a dataset with positive-to-negative ratio $1:200$, i.e., the number of negative samples is $200$ times the number of positive samples.
    A standard technique to combat the imbalance is \emph{Downsampling} and \emph{Upweighting}.
        \begin{itemize}
            \item Downsampling: we sample the negative instances every $10$ times, which results in the new positive-to-negative ratio $1:20$.
            \item Upweighting: after downsampling, the weight of each negative instance needs to be increased by a factor of the downsampling ratio, i.e., $10$.
        \end{itemize}
        
    \paragraph{Cost-Sensitive Learning.}
    Another strategy to combat imbalanced data is through \emph{cost-sensitive} learning.
    Consider binary classification settings.
    Let $\mathcal{Y}_0$ and $\mathcal{Y}_1$ represent the majority and minority classes, respectively. 
    Define the imbalance ratio as follows:
        \begin{equation}
            R = \frac{\SetCard{\mathcal{Y}_1}}{\SetCard{\mathcal{Y}_0}},
        \end{equation}
    which is usually smaller than $1$. 
    Then, the costs of making different types of mistakes become:
        \begin{equation}
            \begin{aligned}
                    \text{Cost(classify a minority sample to majority)} & = 1 \\
                    \text{Cost(classify a majority sample to minority)} & = R.
            \end{aligned}
        \end{equation}
    Intuitively, wrongly classifying a majority sample is less severe than wrongly classifying a minority sample.
        

\subsection{Ranking}
    Consider a set of data $\mathcal{S}=\SET{(\bm{x}_1, \bm{x}'_1, y_1), \ldots, (\bm{x}_{\ndata}, \bm{x}'_{\ndata}, y_{\ndata})}$, where $y_i=+1$ (resp. $-1$) if $\bm{x}'_i$ should be ranked higher (resp. lower) than $\bm{x}_i$.
    Given a hypothesis set $\mathcal{H}$, the ranking problem is to choose a hypothesis $h \in \mathcal{H}$ such that the mis-ranking error is minimized, i.e.,
        \begin{equation}
            R(h) = P_{(\bm{x}_i, \bm{x}'_i) \sim \mathcal{D}}\left[ y_i \cdot \left( h(\bm{x}'_i) - h(\bm{x}_i) \right) \le 0 \right].
        \end{equation}
    
    \subsubsection{Ranking by SVM}
        Let $\mathcal{H}=\Set{\bm{w}}{\bm{w}^\top \Phi(\bm{x})}$, where $\Phi(\bm{x})$ is some feature transformation on $\bm{x}$.
        The problem of ranking is formulated as the following:
            \begin{equation}
                \begin{aligned}
                    & \min_{\bm{w}, \bm{\xi}} & & \frac{1}{2}\norm{\bm{w}}_2^2 + C \sum_{i=1}^{\ndata}{\xi_i} \\
                    & s.t. & & y_i \cdot \left[ \bm{w}^\top \left( \Phi(\bm{x}'_i) - \Phi(\bm{x}_i) \right) \right] \ge 1 - \xi_i, \forall i \\
                    & & & \xi_i \ge 0, \forall i.
                \end{aligned}
            \end{equation}
        The constraints encourage that $\bm{x}'_i$ is ranked higher than $\bm{x}_i$ (i.e., $\bm{w}^\top \left( \Phi(\bm{x}'_i) - \Phi(\bm{x}_i) \right) > 0$) if $y_i=+1$.
        Some cushion $\xi_i$ is allowed, however, its magnitude is penalized by the objective function.


\subsection{Matrix Factorization}
        \paragraph{Low-Rank Model.} 
        The optimization problem is to solve the following:
            \begin{equation}
                \begin{aligned}
                            & \min_{\bm{X}}  & & \norm{\bm{Y} - \bm{X}}_F \\
                            & s.t. & & \text{rank}(\bm{X}) = 1.
                \end{aligned}
            \end{equation}
        In the context of movie recommendation, $Y_{i,j}$ is the ranking of user $i$ to movie $j$.
        We would like to approximate $Y_{i,j} \approx a_i b_j$, where $a_i$ quantifies how generous user $i$ is and $b_j$ measures the  popularity of movie $j$.
        The rank one matrix $\bm{X}$ is such that $\bm{X}=\bm{a}\bm{b}^\top$.
        We use SVD to solve the above optimization, e.g., pick the vectors of $\bm{U}$ and $\bm{V}$ that are associated with the largest singular value.

        \paragraph{Matrix Completion.} 
        Suppose we have a matrix $\bm{Y}$ with some missing entries that we would like to recover. 
        We make the assumption that $\bm{Y}$ is of low rank.
        The problem is formulated as the following:
            \begin{equation}
                \begin{aligned}
                            & \min_{\bm{X} \in \R^{m \times n}} & & \frac{1}{2}\norm{\bm{Y}_\Omega - \bm{X}}_2^2 + \underbrace{\lambda \norm{\bm{X}}_\ast}_{\text{convex relaxation of the low-rank assumption}},
                \end{aligned}
            \end{equation}
        where $\bm{Y}_\Omega$ represents the observed entries; $\norm{\bm{X}}_\ast = \text{trace}(\sqrt{\bm{X}^\top \bm{X}}) = \sum_{i=1}^{\min \SET{m, n}}{\sigma_i(\bm{X})}$ is the nuclear norm, which is a convex relaxation of the number of non-zero singular values (and hence eigenvalues) of $\bm{X}$.
        Notice that the above formulation results from the low-rank assumption.
        We may make other assumptions and the corresponding formulations will be different. 
    
        \paragraph{Scalable Formulation.} 
        The above formulations involving SVD decomposition that are not scalable.
        Instead, we usually solve the following formulation in practice:
            \begin{equation}\label{eq:matrix-factor}
                \begin{aligned}
                            & \min_{\bm{U} \in \R^{m \times d}, \bm{V} \in \R^{n \times d}} & & \sum_{i, j\in \Omega}^{}{ w_{i,j} \big(Y_{i,j} - \langle \bm{u}_i, \bm{v}_j \rangle \big)^2 +  w_0 \sum_{i,j \notin \Omega }^{}{ \left( \langle \bm{u}_i, \bm{v}_j \rangle \right)^2} },
                \end{aligned}
            \end{equation}
        where $\Omega$ is the set of observed entries of $\bm{Y}$.
        The formulation is to do $\bm{Y} \approx \bm{U} \bm{V}^\top$.
        
        \paragraph{Algorithm.} 
        We usually use \emph{weighted alternating least square} (WALS) to solve \eqref{eq:matrix-factor}.
        Intuitively, we alternate between optimizing over either $\bm{U}$ or $\bm{V}$ with the other fixed.
        Check out this tutorial: \url{https://cran.r-project.org/web/packages/rrecsys/vignettes/b5_wALS.html}.
    
    
    
\subsection{Kernel Smoothing Methods}
    \subsubsection{Kernel Density Estimation (KDE)}
        Given a dataset $\mathcal{D}=\SET{\bm{x}_1, \ldots, \bm{x}_\ndata}$, suppose we want to estimate the probability density function  evaluated at a query point $\bm{x}_0$, i.e., $f_\mathcal{D}(\bm{x}_0)$.
        The framework of KDE is as follows:
            \begin{equation}
                f_\mathcal{D}(\bm{x}_0) = \frac{1}{\ndata}\sum_{i=1}^{\ndata}{  \frac{K_\lambda(\bm{x}_i, \bm{x}_0)}{\lambda}   },
            \end{equation}
        where $K_\lambda(\bm{x}_i, \bm{x}_0)$ is some kernel function (e.g., Gaussian kernel) and $\lambda$ is a scale parameter.
        We can think of $\lambda$ as a parameter to control the bias-variance trade-off of the model.
        
    \subsubsection{Kernel Density Classification (KDC)}
        Different from KDE, we need labeled data in KDC.
        Consider a multi-class classification problem with $K$ classes.
        The KDC consists of the following steps:
            \begin{itemize}
                \item We use the KDE framework to estimate a probability density function for each class, i.e., $\hat{f}_k(\bm{x})$ for $k=1,\ldots, K$.
                \item We estimate the proportion of each class based on the labels, i.e., $\hat{\pi}_k$ for $k=1,\ldots, K$.
                \item The probability $\hat{f}_k(\bm{x})$ is $P(\bm{x} | y=k)$ and $\hat{\pi}_k = P(y=k)$, which implies the following by using Bayes Theorem:
                    \begin{equation}
                        P(y=k | \bm{x}) = \frac{P(y=k) P(\bm{x} | y=k)}{P(\bm{x})} = \frac{\hat{\pi}_k \hat{f}_k(\bm{x}) }{\sum_{j=1}^{K}{ \hat{\pi}_j \hat{f}_j(\bm{x})   }}.
                    \end{equation}
            \end{itemize}
            
            
    \subsection{When $\nfeat \gg \ndata$}
        We can use the following techniques to handle the cases where the number of features is a lot larger than the number of data:
            \begin{itemize}
                \item Adding Regularization.
                \item Feature Selection, e.g., Lasso.
                \item Identify non-significant features: for MLE estimators, we can leverage asymptotic normality to do statistical tests.
                Specifically, for a scalar estimator $\hat{\theta}$, we have $\frac{\hat{\theta} - \theta_0 }{\hat{\text{se}}} \sim \mathcal{N}(0, 1)$, where $\theta_0$ is the null hypothesis (usually $0$) and $\hat{\text{se}}$ is the estimated standard error.
                Suppose the null hypothesis is that the parameter is zero, i.e., $\theta_0=0$. 
                We reject the null hypothesis if $\Abs{\frac{\hat{\theta} }{\hat{\text{se}}}} > z_{\alpha/2}$.
                \item Identify non-significant features: likelihood ratio test to check if a subset of parameters should be zeros.
            \end{itemize}
            
    
    \subsection{Recommendation System}
        
        We consider the bipartite analog of a recommender system, where two sets $\mathcal{U}$ and $\mathcal{V}$ represent the users and items, respectively.
        A set of observed user-item interactions is available, i.e., $\Omega$.
        For each user-item pair $(\bm{u}, \bm{v})$, we assume $\bm{u}$ and $\bm{v}$ are embeddings of the user and item, respectively.
        We would like to have a score function $f(\bm{u}, \bm{v})$ to quantify how likely $\bm{u}$ will like $\bm{v}$.
        With such a score function, we can score all unobserved user-item pairs for an user $\bm{w}$ and recommend her the top-$K$ items in terms of the scores.
        
        For a particular user $\bm{w}$, suppose $\mathcal{P}_w$ is the set of items she will like.
        Let $\mathcal{P}_r$ be the $K$ items recommended by the system (thus $\SetCard{\mathcal{P}_r} = K$).
        The performance of the system is measured by \RecallAtK, i.e., 
            \begin{equation}
                \RecallAtK = \frac{\mathcal{P}_w \cap \mathcal{P}_r}{\SetCard{\mathcal{P}_w}}.
            \end{equation}
            
            
        \RecallAtK is not differentiable, so we usually use the following two surrogate functions:
            \begin{itemize}
                \item Binary loss: we pick a set of user-item interactions from $\Omega$ as \emph{positive} instances; similarly, we pick a set of user-item interactions \emph{not} in $\Omega$ as \emph{negative} instances. 
                We then use a sigmoid function to convert the scores to $[0, 1]$ and formulate the learning of the score function as a binary classification problem.
                One issue is that the learned score function is not personalized.
                \item Bayesian personalized ranking (BPR): to achieve personalization,  for each user $u$ BPR defines two sets: 1) the set of positive user-item pairs for $u$, i.e., $\Omega_+(u)=\Set{(u, v)}{(u, v) \in \Omega}$ and 2) the set of negative user-item pairs for $u$, i.e., $\Omega_-(u)=\Set{(u, v)}{(u, v) \notin \Omega}$.
                The BPR loss for $u$ is defined as follows:
                    \begin{equation}
                        L(u) = \frac{1}{\SetCard{\Omega_+(u)} \SetCard{\Omega_-(u)}} \sum_{(u, v_+) \in \Omega_+(u) }^{}{
                            \sum_{(u, v_-) \in \Omega_-(u)}^{}{
                                -\log \Big[ \sigma\big(f_\theta(u, v_+) - f_\theta(u, v_-) \big) \Big]
                            }
                        }.
                    \end{equation}
                One caveat: it is better to have $\SetCard{\Omega_-(u)} > \SetCard{\Omega_+(u)}$, since in practice the number of truly relevant user-item interactions is a lot smaller than that of irrelevant interactions.
            \end{itemize}
            
    
        
        
        
        
    