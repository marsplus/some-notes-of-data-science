

\subsection{A Case Study: Fraud Detection System}
    This section is based on this tutorial: \url{shorturl.at/bxHX0}.
    \begin{enumerate}
        \item Feature extraction: collecting features that are relevant to the problem.
            \begin{itemize}
                \item Credit-card fraud detection: some relevant features include: 1) the features that characterize a user's spending behavior, historical records, geographical attributes, etc. 2) merchants' historical payment records, e.g., typical spending amount, etc. 3) some time windows are needed to aggregate time-sensitive information for both users and merchants. 
            \end{itemize}
        \item Feature transformation: transforming the features to real values.
            \begin{itemize}
                \item Categorical features: binary values or one-hot encoding. 
                \item Date and time: split a time frame (e.g., a day) into several time periods (e.g., morning, afternoon, etc.) and then use one-hot encoding. 
                \item Geographical locations: transform all geographical locations into a unified coordinate system, e.g., the x-y plane.
            \end{itemize}
        \item Train/test/validation data: construct those datasets that are standard in a ML pipeline. 
        \item Model evaluation: choose a suitable metric to evaluate the trained model.
            \begin{itemize}
                \item Threshold-based metric: precision, recall, top-$k$ precision, F1 score.
                \item Threshold-free metric: ROC curve, AUC.
            \end{itemize}
    \end{enumerate}
    
    
\subsection{A Case Study: YouTube Recommendation System}
    This section is based on the paper by~\citet{covington2016deep}.
    As Figure~2 in the paper shows, the system includes two components: 1) candidate generation network and 2) ranking network.
    \paragraph{Candidate Generation Network}
        \begin{itemize}
            \item Goal: generate hundreds of videos out of the massive database of YouTube.
            \item Method: suppose the YouTube dataset $\mathcal{D}$ has one million videos. 
            The paper formulated the candidate generation problem as a binary classification problem, i.e., a video $v_i \in \mathcal{D}$ is in the candidate set iff its predicted label  $y_i = 1$.
            Formally,
                \begin{equation}
                    P(y_i = 1) = \frac{e^{\bm{u}^\top \bm{v}_i}}{ \sum_{j \in \mathcal{D}}^{}{e^{\bm{u}^\top \bm{v}_j}}},
                \end{equation}
            where $\bm{u}$ is the user's embedding and $\bm{v}_i$ the video's embedding.
            \item Feature: users' watched videos/search tokens, demographic information, etc.
            \item Label: 1) explicit feedback, including clicks/likes/shares, etc; 2) implicit feedback, including whether the user watches the entire video, etc.
            \item Training: the objective of training is to learn users' embedding.
            All features are embedded into real-valued space.
            \item Imbalanced data: the training labels are likely to be imbalanced, e.g., only a tiny amount of videos is clicked by the user (i.e., positive examples are rare). 
            So it is necessary to downsample and reweight the negative examples.
            \item Inference: given a user's embedding, the paper used ANN to find the $K$ relevant videos as candidates. 
        \end{itemize}
    \paragraph{Ranking Network}
        \begin{itemize}
            \item Goal: given a user, suppose the candidate generation outputs a set $\mathcal{C}$ of potentially relevant videos.
            The ranking network is to rank the videos in $\mathcal{C}$ and output the top-$k$ videos for the user.
            \item Method: the paper formulated this as a classic ranking problem.
            \item Feature: the embedding of watched videos/impression videos/languages/demographic information, etc.
            \item Label: whether the user clicked the video.
            \item Training: a training data can be a triplet $(v_{i1}, v_{i2}, y_i)$, where $y_i=1$ means video $v_{i1}$ should be ranked higher than video $v_{i2}$, and vice versa.
            \item Inference: given a set of new videos, the ranking network scores each one of them and returns the top-$k$ scored videos.
        \end{itemize}
    \paragraph{Evaluation}
        \begin{itemize}
            \item Offline: typical metrics include precision/recall/ranking loss, etc.
            \item Online: A/B testing.
            \item Offline v.s. Online: the metrics of offline evaluation may not always correlate with online performance.
            In other words, offline metrics are only surrogates of actual performance.
            \item Evaluating the ranking results: NDCG.
        \end{itemize}
        
        
\subsection{A Case Study: Airbnb Personalized Ranking System}
    This section is based on Airbnb's engineering blog: \url{shorturl.at/qAQZ8}.




\subsection{A Case Study: Recommendation System}
    Useful feature categories: 1) users' profiles; 2) items' profiles; 3) users' statistics; 4) items' statistics; 5) contextual features. 
    
        \subsubsection{Evaluation Metric.}
            \begin{itemize}
                \item $\text{Precision@k} = \frac{\text{the number of true positives in the first $k$ items}}{k}$.
                \item $\text{Recall@k} = \frac{\text{the number of true positives in the first $k$ items}}{\text{the total number of positives}}$.
                \item Mean Average Precision (MAP): Suppose $k$ items are recommended to a user. The $i$-th and the $j$-th items are true positives. The average precision for the user is: $\frac{\text{precision@i + precision@j}}{2}$. The MAP is calculated for a set of users as: $\frac{\sum_{u=1}^{n}{\text{average precision of $u$}}}{n}$.
                \item Normalized Discounted Cumulative Gain (NDCG): Suppose five items are recommended to a user. 
                      The 2nd and the 3rd items are true positives.
                      The DCG  is: $\frac{1}{\log{(1 + 2)}} + \frac{1}{\log{(1 + 3)}}$.
                      The Ideal DCG is when the true positives were placed at the top, i.e., the 1st and the 2nd positions, formally $\frac{1}{\log{(1 + 1)}} + \frac{1}{\log{(1 + 2)}}$.
                      The NDCG is DCG divided by IDCG. 
            \end{itemize}

    \subsubsection{Candidate Retrieval}
        There are two neural networks (a.k.a. two towers) $f$ and $g$ that learn the users' embeddings and the items' embeddings, respectively.
        The user tower $f$ maps the attributes of an user to a real-valued vector, i.e., $f(\text{user}_i) = \bm{u}_i$.
        Similarly, the item tower $g$ maps the attributes of an item to a real-valued vector, i.e., $g(\text{item}_j) = \bm{v}_j$.
        The two-tower model has wide applications in: 1) embedding learning, 2) candidate retrieval and 3) ranking. 

        \paragraph{Positive Samples.}
            These are the items that were shown to the users and were clicked.
            It is possible that most of such items are popular items. 
            Thus, when constructing the set of positives, we need to downsample these popular items (e.g., sampling probability inversely proportional to the popularity) and upsample not-so-popular items. 

        \paragraph{Negative Samples.}
                \begin{enumerate}
                    \item Easy negatives: sampling  from the entire dataset with probability: $P(\text{sample item $i$ as a negative}) \propto (\text{popularity of $i$})^{0.75}$. The sampling is not uniform, and this is specifically designed for sampling more popular items as negatives. 
                    
                    \item Batch negatives: given a batch of positives $\SET{(\bm{u}_i, \bm{v}_i^+)}_{i=1}^n$, all $\bm{v}_k^+$ where $k \ne i$ can be treated as the negatives for $\bm{u}_i$. 
                    Notice that the training objective is adapted to $\text{cos}(\bm{u}_i, \bm{v}_k^-) \textcolor{red}{- \log p_k}$, where $p_k$ is the probability of sampling $\bm{v}_k^-$ from the entire dataset.
                    \item Hard negatives: these items that are retrieved by the candidate retrieval model but are ranked low by the ranking model. 
                \end{enumerate}
    
        \paragraph{Training Methods.}
            Given a user $\bm{u}_i$, a positive sample $\bm{v}_j^+$, and a negative sample $\bm{v}_j^-$, there are usually three methods for training the two-tower model.
            \begin{itemize}
                \item Pointwise training: treat the problem as a supervised learning problem. 
                The objective is making $\text{cosine}(\bm{u}_i, \bm{v}_j^+)$ close to 1 and $\text{cosine}(\bm{u}_i, \bm{v}_j^-)$ close to 0. 
                The ratio of positives versus negatives is typically $1:2$ or $1:3$.
                
                \item Pairwise training: each training sample is a triplet, i.e., $(\bm{u}_i, \bm{v}_j^+, \bm{v}_j^-)$. Use triplet hinge/logistic loss to do the training. 
                Notice that the network architecture needs to be adapted to have three towers, one for each sample in the triplet. 
                
                \item Listwise: each training sample is a list $(\bm{u}_i, \bm{v}_j^+, \bm{v}_1^-, \ldots, \bm{v}_j^-)$. The resulting problem is a multi-classification problem.
            \end{itemize}
    

    \subsubsection{Ranking Model}
        For a particular user, five types of features can be used for ranking: 1) the user's profile, including the user's embedding, etc. 2) items' profiles, including the item's embedding; 3) the user's statistics, e.g., recent activities; 4) the item's statistics, e.g., recent order history; 5) contextual features, e.g., the day of week of the session. 
        
        These features are concatenated into a long vector, i.e., $\bm{x} = [\bm{u}, \bm{c}, \bm{s}, \bm{v}]$.
        The ranking model $f$ has a couple of heads. Each head is a neural network (e.g., an MLP with a sigmoid output) that learns a particular target.
        For example, the targets can be $f(\bm{x}) = [y_1, y_2, y_3]$, where $y_1$ is the rate of likes, $y_2$ is the rate of shares, and $y_3$ is the rate of conversion, etc. 
        Finally, the three quantities are combined to generate a final score for the item, e.g., by a linear combination of $y_1$, $y_2$ and $y_3$.

        \paragraph{Training.}
        A cross-entropy loss is used, quantifying the differences between the outputs of the heads and the ground-truth.
        One issue is data imbalance, where the number of negatives are way more than the number of positives. 
        The solution is down-sampling the negatives, e.g., sampling only $\alpha \in (0, 1)$ of the negatives. 
        Due to the down-sampling, the estimated probabilities from the heads are overestimated. 
        Use the probability of clicks as an example. 
        Suppose the true probability is $p = \frac{n^+}{n^+ + n^-}$, where $n^+$ and $n^-$ are clicks and non-clicks, respectively. 
        The estimated probability is $\hat{p} = \frac{n^+}{n^+ + \alpha n^-} > p$.
        To get the calibrated estimation we transform $\hat{p}$ as follows: $\frac{\alpha \hat{p}}{(1 - \hat{p}) + \alpha \hat{p}}$.
        
    

    \subsection{Leveraging Historical Behavior}
        For an user $u_i$, suppose she has liked $n$ items, and the embeddings of the $n$ items are $\bm{v}_1, \ldots, \bm{v}_n$.
        These $n$ embeddings are combined into a single vector, representing the user's past behavior, e.g., through the average $(1/n)\sum_{i=1}^{n}{\bm{v}_i}$.
        Alternatively, when ranking a particular candidate item, an attention layer can be used to learn a linear combination of the $n$ vectors, where the attention coefficients are defined as the similarity measures between the candidate item and the $n$ items. 