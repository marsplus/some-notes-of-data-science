

\subsection{Linear Programming (LP)}
    Some useful things about LP:
    \begin{itemize}
        \item Polyhedron: A polyhedron $\mathcal{P}$ is a set that can be described by \Set{\bm{x} \in \R^n}{\bm{A} \bm{x} \ge \bm{b}}.
        \item Extreme point: A point $\bm{x}$ is an extreme point of a polyhedron $\mathcal{P}$ if we cannot find two other points $\bm{y}, \bm{z}$ (different from $\bm{x}$) and a scalar $\lambda \in [0, 1]$ such that $\bm{x} = \lambda \bm{y} + (1-\lambda) \bm{z}$.
        \item An LP attains optima at extreme points; see \cite{bertsimas1997introduction} at page 65 for a proof.
        \item In general, the time complexity of solving an LP is $O(n^{3.5})$, where $n$ is the number of variables; see \url{https://en.wikipedia.org/wiki/Karmarkar%27s_algorithm}. 
    \end{itemize}

    \paragraph{Deriving the Dual Form.}
    An primal LP in standard form is as follows:
        \begin{equation}
            \begin{aligned}
                & \min_{x_1, x_2} & &   \textcolor{blue}{4} x_1 + \textcolor{blue}{3} x_2 \\
                & s.t.            &  & 2 x_1 + x_2 \ge \textcolor{red}{3} \\
                &                 &  & x_1 + 2 x_2 \ge \textcolor{red}{2} \\
                &                 &  & x_1 \ge 0, x_2 \ge 0.
            \end{aligned}
        \end{equation}
    The objective function of the dual problem is a lower bound of the primal objective, which is obtained by linear combination of the primal constraints, i.e., 
        \begin{equation}
            \begin{aligned}
                & \max_{y_1, y_2} & &   \textcolor{red}{3} y_1 + \textcolor{red}{2} y_2\\
                & s.t.            &  & 2 y_1 + y_2 \le \textcolor{blue}{4} \\
                &                 &  & y_1 + 2 y_2 \le \textcolor{blue}{3} \\
                &                 &  & y_1 \ge 0, y_2 \ge 0.
            \end{aligned}
        \end{equation}
    
    
    
    
\subsection{Convex Optimization}
    \subsubsection{Duality Theory}
    
    \subsubsection{Slater's Condition}
    
    \subsubsection{Gradient Descent Method}
    \begin{itemize}
        \item Convergence rate for unconstrained, smooth convex programming: $O(1/k)$, where $k$ is the number of iterations; see page 11 at \url{https://www.stat.cmu.edu/~ryantibs/convexopt/scribes/grad-descent-scribed.pdf}.
        \item Convergence rate for unconstrained, smooth and \emph{strongly} convex programming: $O(\gamma^k)$, where $k$ is the number of iterations and $0 < \gamma < 1$.
    \end{itemize}
    
    \subsubsection{Subgradient Methods}
    Given a function $f(\bm{x}): \R^\nfeat \rightarrow \R$ that is not differentiable at $\bm{x}_0$. 
    The set of subgradients at $\bm{x}_0$ is denoted by $\partial f(\bm{x}_0)$, formally
        \begin{equation}
            \partial f(\bm{x}_0) = \Set{\bm{g} \in \R^\nfeat}{f(\bm{x}) \ge f(\bm{x}_0) + \bm{g}^\top (\bm{x} - \bm{x}_0), \forall \bm{x}}.
        \end{equation}
    Intuitively, this is the set of directions that never over-estimate the value of $f(\bm{x})$.
    Some examples of subgradients are as follows:
        \begin{itemize}
            \item $f(\bm{x}) = \norm{\bm{x}}_2$:
                \begin{equation}
                    \partial f(\bm{x}) = \begin{cases}
                        \frac{\bm{x}}{\norm{\bm{x}}_2} & , \bm{x} \ne \bm{0},\\
                        \Set{\bm{z}}{\norm{\bm{z}}_2 \le 1} &, \bm{x} = \bm{0}.
                    \end{cases}
                \end{equation}
            This is because: for $\bm{x} \ne \bm{0}$, it follows that $\norm{\bm{y}}_2 \ge \norm{\bm{x}}_2 + \left(\frac{\bm{x}}{\norm{\bm{x}}_2}\right)^\top (\bm{y} - \bm{x}) \iff \norm{\bm{y}}_2 \norm{\bm{x}}_2 \ge \bm{x}^\top \bm{y}$, which is true by Cauchy-Schwarz inequality.
            For $\bm{x} = \bm{0}$, we have $\partial f(\bm{x}) = \Set{\bm{z}}{\norm{\bm{y}}_2 \ge \bm{z}^\top \bm{y}}$
            \item $f(\bm{x}) = \norm{\bm{x}}_1$:
            \item $f(\bm{x}) = \mathbbm{I}_\mathcal{C}[\bm{x}]$ (indicator function of the set $\mathcal{C}$):
        \end{itemize}
    
    \subsubsection{Proximal (or Projected) Gradient Descent (PGD)}
    \begin{itemize}
        \item PGD is often used to solve the following optimization problem
            \begin{equation}\label{PGD}
                \argmin_{\bm{\beta}} g(\bm{\beta}) + h(\bm{\beta}),
            \end{equation}
        where $g$ is smooth and convex while $h$ is convex but not smooth.  
        Note that we may use subgradient method, however, the convergence rate is bad.
        The rate of convergence is $O(1/k^2)$, which is a lot worse than $O(1/k)$, i.e., the rate of convergence for gradient descent.
        \item The definition of the proximal mapping:
            \begin{equation}
                \text{prox}_h(\bm{x}) = \argmin_{\bm{z}} h(\bm{z}) + \frac{1}{2}\norm{\bm{x} - \bm{z}}^2_2.
            \end{equation}
        \item For a couple of instantiations of $h$, the corresponding proximal mapping can be efficiently computed:
            \begin{enumerate}
                \item $h(\bm{x}) = \mathbbm{1}[\bm{x} \in \mathcal{C}]$: the indicator function of whether $\bm{x}$ is in $\mathcal{C}$. The proximal mapping simplifies to $\argmin_{\bm{z}}\norm{\bm{x} - \bm{z}}$. When $\mathcal{C}$ is a convex set, the problem of proximal mapping becomes a convex programming.
                \item $h(\bm{x}) = t \norm{\bm{x}}_1$: the $L_1$ regularization on $\bm{x}$. In this case, the resulting proximal mapping is the famous \emph{soft thresholding} (which is the algorithm to solve Lasso), i.e., 
                    \begin{equation}
                        \text{prox}_h(\bm{x})_i = \begin{cases}
                            x_i - t,&  x_i > t \\
                            0, & -t < x_i < t \\
                            x_i + t, & x_i < -t
                        \end{cases}.
                    \end{equation}
            \end{enumerate}
            
            
        \item The iterative step of the proximal gradient method to solve \eqref{PGD} is as follows:
            \begin{equation}
                    \bm{x}^t = \text{prox}_{h, t_k}\left( \bm{x}^{t-1} - t_k \nabla g(\bm{x}^{t-1}) \right).
            \end{equation}
        The intuition of the iterative step is to approximate $g(\bm{\beta})$ with its second-order Taylor expansion evaluated at $\bm{x}^{t-1}$.
        See the notes for details: \url{https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture18.pdf}.
    \end{itemize}
    
\subsection{Matrix Norms}
    % \begin{itemize}
    %     \item 
    % \end{itemize}