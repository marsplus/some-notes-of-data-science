

\subsection{Linear Programming (LP)}
    Some useful things about LP:
    \begin{itemize}
        \item Polyhedron: A polyhedron $\mathcal{P}$ is a set that can be described by \Set{\bm{x} \in \R^n}{\bm{A} \bm{x} \ge \bm{b}}.
        \item Extreme point: A point $\bm{x}$ is an extreme point of a polyhedron $\mathcal{P}$ if we cannot find two other points $\bm{y}, \bm{z}$ (different from $\bm{x}$) and a scalar $\lambda \in [0, 1]$ such that $\bm{x} = \lambda \bm{y} + (1-\lambda) \bm{z}$.
        \item An LP attains optima at extreme points; see \cite{bertsimas1997introduction} at page 65 for a proof.
        \item In general, the time complexity of solving an LP is $O(n^{3.5})$, where $n$ is the number of variables; see \url{https://en.wikipedia.org/wiki/Karmarkar%27s_algorithm}. 
    \end{itemize}
    
    
    
\subsection{Convex Optimization}
    \subsubsection{Duality Theory}
    
    \subsubsection{Slater's Condition}
    
    \subsubsection{Gradient Descent Method}
    \begin{itemize}
        \item Convergence rate for unconstrained, smooth convex programming: $O(1/k)$, where $k$ is the number of iterations; see page 11 at \url{https://www.stat.cmu.edu/~ryantibs/convexopt/scribes/grad-descent-scribed.pdf}.
        \item Convergence rate for unconstrained, smooth and \emph{strongly} convex programming: $O(\gamma^k)$, where $k$ is the number of iterations and $0 < \gamma < 1$.
    \end{itemize}
    
    \subsubsection{Subgradient Methods}
    Given a function $f(\bm{x}): \R^\nfeat \rightarrow \R$ that is not differentiable at $\bm{x}_0$. 
    The set of subgradients at $\bm{x}_0$ is denoted by $\partial f(\bm{x}_0)$, formally
        \begin{equation}
            \partial f(\bm{x}_0) = \Set{\bm{g} \in \R^\nfeat}{f(\bm{x}) \ge f(\bm{x}_0) + \bm{g}^\top (\bm{x} - \bm{x}_0), \forall \bm{x}}.
        \end{equation}
    Intuitively, this is the set of directions that never over-estimate the value of $f(\bm{x})$.
    
    \subsubsection{Proximal (or Projected) Gradient Descent (PGD)}
    \begin{itemize}
        \item PGD is often used to solve the following optimization problem
            \begin{equation}\label{PGD}
                \argmin_{\bm{\beta}} g(\bm{\beta}) + h(\bm{\beta}),
            \end{equation}
        where $g$ is smooth and convex while $h$ is convex but not smooth.  
        Note that we may use subgradient method, however, the convergence rate is bad.
        The rate of convergence is $O(1/k^2)$, which is a lot worse than $O(1/k)$, i.e., the rate of convergence for gradient descent.
        \item The definition of the proximal mapping:
            \begin{equation}
                \text{prox}_h(\bm{x}) = \argmin_{\bm{z}} h(\bm{z}) + \frac{1}{2}\norm{\bm{x} - \bm{z}}^2_2.
            \end{equation}
        \item For a couple of instantiations of $h$, the corresponding proximal mapping can be efficiently computed:
            \begin{enumerate}
                \item $h(\bm{x}) = \mathbbm{1}[\bm{x} \in \mathcal{C}]$: the indicator function of whether $\bm{x}$ is in $\mathcal{C}$. The proximal mapping simplifies to $\argmin_{\bm{z}}\norm{\bm{x} - \bm{z}}$. When $\mathcal{C}$ is a convex set, the problem of proximal mapping becomes a convex programming.
                \item $h(\bm{x}) = t \norm{\bm{x}}_1$: the $L_1$ regularization on $\bm{x}$. In this case, the resulting proximal mapping is the famous \emph{soft thresholding} (which is the algorithm to solve Lasso), i.e., 
                    \begin{equation}
                        \text{prox}_h(\bm{x})_i = \begin{cases}
                            x_i - t,&  x_i > t \\
                            0, & -t < x_i < t \\
                            x_i + t, & x_i < -t
                        \end{cases}.
                    \end{equation}
            \end{enumerate}
            
            
        \item The iterative step of the proximal gradient method to solve \eqref{PGD} is as follows:
            \begin{equation}
                    \bm{x}^t = \text{prox}_{h, t_k}\left( \bm{x}^{t-1} - t_k \nabla g(\bm{x}^{t-1}) \right).
            \end{equation}
        The intuition of the iterative step is to approximate $g(\bm{\beta})$ with its second-order Taylor expansion evaluaated at $\bm{x}^{t-1}$.
        See the note for details: \url{https://people.eecs.berkeley.edu/~elghaoui/Teaching/EE227A/lecture18.pdf}.
    \end{itemize}
    
\subsection{Matrix Norms}
    \begin{itemize}
        \item 
    \end{itemize}